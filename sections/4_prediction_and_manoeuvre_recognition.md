# `Prediction` and `Manoeuvre Recognition`

- Questions when developing a **predictor**:
  - Which **application**?
    - _Who?_ `pedestrians` or `vehicles`?
    - _Where?_ `highway` or `urban`?
  - Scenarios: **generic** or **specific**?
    - E.g. address **all types of intersections**, or only one specific `T`-intersection?
  - Which prediction **horizon**?
    - Usually `physics`-based for **short term** and `goal`-based for **longer horizons**.
  - **Frequency** of `observation`s?
    - Maybe need to consider a **sequence** of `observation`s.
  - How much **robustness** is needed?
    - Ability to **generalize** / **transfer** well in new environments?
  - **Uncertainty**: how to model **_multi-modal_ futures distributions**?
    - Uncertainty comes from the **stochastic nature** of human motion and the non-perfect **perception**.
    - Be careful with **mode-collapse**.
  - **Context**-aware: which information to consider?
    - About the `target agent`, the `static` and the `dynamic` environment.
    - Consider/ignore other agents?
      - How to model **interactions**?
    - Consider/ignore `map` / `road` structure?
      - Is a `map` available?
  - How many **obstacles**?
    - How to represent them?
    - Can it scale?
  - How much **expert / prior knowledge** to inject? What can be learnt?
    - E.g. the **dynamics** of moving agents.
    - Modern techniques make extensive use of `ML` in order to **better estimate context-dependent patterns** in real-data.
    - What dataset available?
  - How much **computational cost** is constrained?
  - How to obtain a **trajectory**, i.e. a **sequence** of positions?
    - **One-shot** feed-forward: **directly** predicts a **time sequence**.
    - Roll-outs: repeat **one-step** prediction with the **forward propagation** of the model.
      - But this requires a **`transition` function**.
  - What `metrics` and test set for **evaluation**?
    - If possible comparable with other works.

---

**`"TNT: Target-driveN Trajectory Prediction"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2008.08294)]**
**[**:car: `Waymo`**]**

- **[** _`multimodal`, `VectorNet`, `goal-based prediction`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2008.08294).](../media/2020_zhao_1.PNG "[Source](https://arxiv.org/abs/2008.08294).")  |
|:--:|
| *__`TNT`__ = **target**-driven trajectory prediction. The intuition is that the **uncertainty of future states** can be decomposed into two parts: the **`target` or `intent` uncertainty**, such as the decision between turning `left` and `right`; and the **`control` uncertainty**, such as the **fine-grained motion** required to perform a turn. Accordingly, the probabilistic distribution is decomposed by **conditioning on `targets`** and then **marginalizing over them**. [Source](https://arxiv.org/abs/2008.08294).* |

Authors: Zhao, H., Gao, J., Lan, T., Sun, C., Sapp, B., Varadarajan, B., Shen, Y., Shen, Y., Chai, Y., Schmid, C., Li, C., & Anguelov, D.

- One sentence:
  - > "Our key insight is that for **prediction** within a **moderate time horizon**, the **future modes** can be effectively captured by a **set of target `states`**."

- Motivations:
  - `1-` Model **multimodal futures distributions**.
  - `2-` Be able to **incorporate expert knowledge**.
  - `3-` Do not rely on **run-time sampling** to estimate trajectory distributions.

- _How to model_ **_multimodal futures distributions_**_?_
  - `1-` **Future `modes`** are **implicitly modelled** as **`latent` variables**, which should capture the **underlying `intents` of the agents**.
    - Diverse trajectories can be **generated by sampling** from these **implicit distributions**.
    - E.g. `CVAE` in [`DESIRE`](https://arxiv.org/abs/1704.04394), `GAN` in [`SocialGAN`](https://arxiv.org/abs/1803.10892), single-step policy **roll-out** methods: they are prone to **_mode collapse_** too.
    - Issue: the use of **`latent` variables to model intents** prohibits them to be **interpreted**. Incorporating **expert knowledge** is made challenging.
    - Issue: it often requires **test-time sampling** [stochastic sampling from the `latent` space] to evaluate **probabilistic queries** (e.g., _‚Äúhow likely is the agent to turn left?‚Äù_) and obtain **implicit distributions**.
    - > "Modeling the future as a **discrete set of targets** does not suffer from **mode averaging**, which is the major factor that hampers **multimodal predictions**."
  - `2-` **Decompose** the trajectory prediction task **into subtasks**.
    - For [instance](https://www.ri.cmu.edu/pub_files/2009/10/planning-based-prediction-pedestrians.pdf) with **`planning`-based prediction**:
      - First estimate a Bayesian posterior **distribution of destinations**.
      - Then used `IRL` to plan the trajectories.
    - Or by decomposing **_`goal` distribution estimation_** and **_`goal`-directed `planning`_**.
  - `3-` Discretize the **output space** as **intents** or with **anchors**.
    - [`IntentNet`](https://proceedings.mlr.press/v87/casas18a/casas18a.pdf) [`UBER`]: several common **`motion` categories** are manually defined (e.g. `left turn` and `lane changes`) and a **separate motion predictor** is learnt for each `intent`.
    - > "[`MultiPath`](https://arxiv.org/abs/1910.05449) [`Waymo`] and [`CoverNet`](https://arxiv.org/pdf/1911.10298.pdf) [`nuTonomy`] chose to **quantize** the trajectories into **`anchors`**, where the trajectory prediction task is reformulated into **`anchor` selection** and **offset regression**."
    - > "Unlike **`anchor`** trajectories, the **targets** in `TNT` are much **lower dimensional** and can be **easily discretized** via _uniform sampling_ or based on _expert knowledge_ (e.g. HD maps). Hence, they can be estimated more reliably."

- **`TNT`** = **target**-driven trajectory prediction.
  - The framework has **`3` stages** that are **trained end-to-end**:
  - `1-` **`Target` prediction**.
    - Estimate a **distribution** over **candidate `targets`**, `T` steps into the future, given the encoded `scene context`.
    - The potential future `targets` are modelled via a set of `N` discrete, **quantized locations** with **continuous offsets**.
      - > "We can see that with **regression** the performance improved by `0.16m`, which shows the necessity of **position refinement** from the **original target coordinates**."
  - `2-` **`Target`-conditioned motion estimation**.
    - Predict **trajectory `state` sequences**  conditioned on `targets`.
    - Two assumptions:
      - `1-` Future time steps are **conditionally independent**. **Sequential predictions** are therefore avoided.
      - `2-` The **distribution** of the trajectories is **unimodal (`normal`) given the target**.
  - `3-` **Scoring and selection**.
    - Estimates the **likelihood of each predicted trajectory**, taking into account the `context` of all other predicted trajectories
      - > "Our final stage estimates **the `likelihood` of full future trajectories `sF`**. This differs from the second stage, which **decomposes over time steps and `targets`**, and from the first stage which **only has knowledge of `targets`**, but not full trajectories ‚Äî e.g., a `target` might be estimated to have high `likelihood`, but a full trajectory to reach that target might not."
    - Select a **final compact set** of trajectory predictions.
      - > "This process is inspired by the **non-maximum suppression** algorithm commonly used for computer vision problems, such as object detection."
  
- _How is the `context` information encoded, i.e. the ego-car's interactions with the environment and the other agents?_
  - When the HD map is available: Using the **hierarchical graph neural network [`VectorNet`](https://arxiv.org/abs/2005.04259)** [`Waymo`].
    - > "Polylines are used to abstract the **HD map elements** `cP` (lanes, traffic signs) and **agent trajectories `sP`**; a **subgraph network** is applied to encode each polyline, which contains a variable number of vectors; then a **global graph** is used to **model the interactions between polylines**."
  - > " If **`scene context`** is only available in the form of **top-down imagery**, a `ConvNet` is used as the **context encoder**."
- _How to generate the `targets`, i.e. design the `target space`?_
  - The **`target space`** is approximated by a **set of discrete locations**. Expert knowledge (such as **road topology**) can be incorporated there, for instance by **sampling `targets` on and around the lanes**."
    - > "These `targets` are not only grounded in **physical entities that are interpretable** (e.g. `location`), but also correlate well with **`intent`** (e.g. a `lane change` or a `right turn`)."
  - `1-` For **vehicle**: points are uniformly sampled on **lane centerlines** from the HD map and used as **`target` candidates**, with the assumption that **vehicles never depart far away from lanes**.
  - `2-` For **pedestrians**: a **virtual grid** is generated around the agent and use the **grid points as target candidates**.
    - Grid of range **`20m`x`20m`** with a **grid size of `0.5m`** -> **`1600` targets** are considered and only the best `50` are kept for further processing.
- _What is the ground truth?_
  - _Not very clear to me how you can use a single demonstration as an oracle: today you `turn left`, but yesterday, with same `context`, you `turned right`._
  - > "The `ground truth score` of each predicted trajectory is defined by its **distance to ground truth trajectory `œà`(`sF`)**."

- Results:
  - **`TNT` outperforms the state-of-the-art** on prediction of:
    - `1-` **Vehicles**: [`Argoverse`](https://www.argoverse.org/) Forecasting and [`INTERACTION`](http://interaction-dataset.com/).
    - `2-` **Pedestrian**: [`Stanford Drone`](https://cvgl.stanford.edu/projects/uav_data/) and an in-house Pedestrian-at-Intersection dataset (`PAID`).
  - For benchmark, `MultiPath` and `DESIRE` are reimplemented by replacing their `ConvNet` context encoders with `VectorNet`.

</details>

---

**`"Modeling and Prediction of Human Driver Behavior: A Survey"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2006.08832)]**
**[** :mortar_board: `Stanford`, `University of Illinois` **]**
**[**:car: `Qualcomm`**]**

- **[** _`state estimation`, `intention estimation`, `trait estimation`, `motion prediction`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2006.08832).](../media/2020_brown_1.PNG "[Source](https://arxiv.org/abs/2006.08832).")  |
|:--:|
| *Terminology: the problem is formulated as a **discrete-time multi-agent partially observable stochastic game** (`POSG`). In particular, the `internal state` can contain `agent‚Äôs navigational goals` or the `behavioural traits`. [Source](https://arxiv.org/abs/2006.08832).* |

Authors: Brown, K., Driggs-Campbell, K., & Kochenderfer, M. J.

- Motivation:
  - A **review and taxonomy** of `200` models from the literature on **driver behaviour modelling**.
    - > "In the context of the **partially observable stochastic game (`POSG`) formulation**, a **`driver behavior model`** is a collection of **assumptions** about the `human observation` function **`G`**, `internal-state update` function **`H`** and `policy` function **`œÄ`** (the `state-transition` function **`F`** also plays an important role in driver-modeling applications, though it has **more to do with the vehicle than the driver**)."
    - _The "References" section is large and good!_
  - Models are categorized based on the **tasks** they aim to address.
    - `1-` **`state` estimation**.
    - `2-` **`intention` estimation**.
    - `3-` **`trait` estimation**.
    - `4-` **`motion` prediction**.
- The following lists are **non-exhaustive**, see the tables for full details. Instead, they try to give an overview of the **most represented** instances:
- `1-` **(Physical) `state` estimation**.
  - **[Algorithm]**: approximate **recursive Bayesian filters**. E.g. `KF`, `PF`, `moving average filter`.
  - > "Some advanced **`state` estimation** models take advantage of the **structure inherent in the driving environment** to improve filtering accuracy. E.g. `DBN`".

- `2-` **(Internal states) `intention` estimation**.
  - > "`Intention` estimation usually involves **computing a probability distribution** over a **finite set of possible behavior modes** - often corresponding to **navigational goals** (e.g., `change lanes`, `overtake`) - that a driver might execute in the current situation."
  - **[Architecture]**: `[D]BN`, `SVM`, `HMM`, `LSTM`.
  - **[Scope]**: `highway`, `intersection`, `lane-changing`, `urban`.
  - **[Evaluation]**: `accuracy` (classification), `ROC` curve, `F1`, `false positive rate`.
  - **[Intention Space]** (_set of possible_ **_behaviour modes_** _that may exist in a driver‚Äôs `internal state`_ - often **combined**):
    - **`lateral` modes** (e.g. `lane-change` / `lane-keeping` intentions),
    - **`routes`** (a sequence of decisions, e.g. `turn right ‚Üí go straight ‚Üí turn right again` that a driver may intend to execute),
    - `longitudinal` modes (e.g. `car-following` / `cruising`),
    - joint `configurations`.
    - > "**`Configuration` intentions** are defined in terms of **spatial relationships** to **other vehicles**. For example, `intention estimation` for a `merging` scenario might involve reasoning about **_which gap between vehicles the target car intends to enter_**. The `intention space` of a car in the other lane might be **_whether or not to yield_** and allow the merging vehicle to enter."
  - **[Hypothesis Representation]** (_how to represent uncertainty in the intention hypothesis?_): **`discrete probability distribution`** over possible `intentions`.
    - > "In contrast, **`point estimate`** hypothesis **ignores uncertainty** and simply assigns a **probability of `1`** to a single (presumably the **most likely**) behavior mode."
  - **[Estimation / Inference Paradigm]**: `single-shot`, `recursive`, `Bayesian` (based on **probabilistic graphical models**), `black-box`, `game theory`.
    - > "**`Recursive`** estimation algorithms operate by **repeatedly updating** the intention hypothesis **at each time step** based on the new information received. In contrast, **`single-shot`** estimators compute a **new hypothesis from scratch** at each inference step. The latter may operate over a **history of observations**, but it **does not store** any information between successive **inference iterations**."
    - > "**Game-theoretic** models are distinguished by being **`interaction-aware`**. They **explicitly consider possible situational outcomes** in order to compute or refine an intention hypothesis. This **`interaction-awareness`** can be as simple as **pruning intentions** with a high probability of **conflicting with other drivers**, or it can mean computing the **Nash equilibrium** of an explicitly formulated game with a **payoff matrix**."

- `3-` **`trait` estimation**.
  - > "Whereas **`intention` estimation** reasons about **_what a driver is trying to do_**, **`trait estimation`** reasons about **_factors that affect how the driver will do it_**. Broadly speaking, traits encompass `skills`, `preferences`, and `style`, as well as properties like `fatigue`, `distractedness`, etc."
  - > "**`Trait` estimation** may be interpreted as the process of **inferring the ‚Äúparameters‚Äù of the driver‚Äôs `policy function œÄ`** on the basis of observed driving behavior. [...] `Traits` can also be interpreted as part of the **driver‚Äôs internal state**."
  - **[Architecture]**: `IDM`, `MOBIL`, `reward` parameters.
  - **[Training]**: `IRL`, `EM`, `genetic algorithms`, `heuristic`.
  - **[Theory]**: Inverse `RL`.
  - **[Scope]**: `car following` (`IDM`), `highway`, `intersection`, `urban`.
  
  - **[Trait Space]**: `policy` parameters, `reward` parameters (assuming that drivers are trying to optimize a `cost` function).
    - > "Some of the most widely known driver models are **simple parametric controllers** with **tuneable ‚Äústyle‚Äù or ‚Äúpreference‚Äù `policy` parameters** that represent intuitive **behavioral `traits`** of drivers. E.g. `IDM`."
    - **`IDM` traits**: _`minimum desired gap`, `desired time headway`, `maximum feasible acceleration`, `preferred deceleration`, `maximum desired speed`_.
    - > "**`Reward` function parameters** often correspond to the same intuitive notions mentioned above (e.g., `preferred velocity`), the important difference being that they parametrize a `reward` function rather than a **closed-loop control `policy`**."
  - **[Hypothesis Representation]** (uncertainty): in almost all cases, the hypothesis is represented by **a `point estimate`** rather than a **`distribution`**.
  - **[Estimation Paradigm]**: `offline` / `online`.
    - > "Some models combine the two paradigms by computing a **prior distribution `offline`**, then **tuning it `online`**. This tuning procedure often relies on **Bayesian** methods."
  - **[Model Class]**: **`heuristic`**, `optimization`, `Bayesian`, `IRL`, **`contextually varying`**.
    - > "One simple approach to **`offline`** trait estimation is to set `trait` parameters **heuristically**. Specifying parameters **manually** is one way to **incorporate expert domain knowledge** into models."
    - > "In some approaches, `trait` parameters are modeled as **contextually varying**, meaning that they vary based on the region of the `state` space (the **context**) or the **current behavior mode**."

- `4-` **`motion` prediction**.
  - > "Infer the future **physical `states`** of the surrounding vehicles".
  - **[Architecture]**: `IDM`, `LSTM` (and other `RNN`/`NN`), constant acceleration / speed (`CA`, `CV`), `encoder-decoder`, `GMM`, `GP`, `adaptive`, `spline`.
  - **[Training]**: `heuristic`.
    - > "Simple examples include **rule-based heuristic control** laws like `IDM`. More sophisticated examples include **closed-loop policies** based on `NN`, `DBN`, and random forests."
  - **[Theory]**: `RL`, `MPC`, trajectory optimization.
    - > "Some `MPC` policy models (including those used within a **forward simulation** paradigm) fall into the **game theoretic** category because they **explicitly predict the future states** of their environment (including other cars) before computing a planned trajectory."
  - **[Scope]**: `highway`, `car-following` (e.g. using `IDM`), `intersection`, `urban`.
  - **[Evaluation]**: `RMSE`, `NLL`, `MAE`, `collision rate`.
  - **[Vehicle dynamics model]**: `linear`, `learned`, `bicycle kinematic`.
    - > "Many models in the literature assume **linear `state-transition` dynamics**. Linear models can be **first order** (i.e., `output` is `position`, `input` is `velocity`), **second order** (i.e., `output` is `position`, `input` is `acceleration`), and so forth."
    - > "**Kinematic** models are simpler than **dynamic** models, but the **no-slip assumption** can lead to significant modeling errors."
    - > "Some `state-transition` models are **learned**, in the sense that the **observed correlation** between consecutive **predicted `states`** results entirely from training on **large datasets**. Some incorporate an explicit transition model where the **parameters are learned**, whereas others simply **output a full trajectory**."
  - **[`Scene`-level _uncertainty_ modelling]**:
    - `single-scenario` (ignoring **multimodal uncertainty** at the scene level),
    - `partial scenario`,
    - `multi-scenario` (reason about the different possible scenarios that **may follow** from an initial traffic scene),
    - `reachable set`.
    - > "Some models reason only about a **partial scenario**, meaning they predict the motion of **only a subset** of vehicles in the traffic scene, usually under a single scenario."
    - > "Some models reason about **multimodal uncertainty** on the `scene`-level by performing **multiple (parallel) rollouts** associated with **different scenarios**."
    - > "Rather than reasoning about the **likelihood of future `states`**, some models reason about reachability. **Reachability analysis** implies taking a **worst-case mindset** in terms of predicting vehicle motion."
  - **[`Agent`-level _uncertainty_ modelling]**: `single deterministic`, `particle set`, `Gaussians`.
  - **[Prediction paradigm]**:
    - **open-loop `independent`** trajectory prediction.
      - > "Many models operate under the **independent prediction** paradigm, meaning that they predict a **full trajectory independently** for each agent in the scene.  These approaches are **`interaction-unaware`** because they are **open-loop**. Though they may account for **interaction** between vehicles at the **current time `t`**, they **do not explicitly reason** about interaction over the **prediction window** from `t+1` to the **prediction horizon `tf`**.  [...] Because independent trajectory prediction models **ignore interaction**, their predictive power tends to quickly **degrade** as the **prediction horizon extends further** into the future."
    - **closed-loop `forward`** simulation.
      - > "In the **forward simulation** paradigm, motion hypotheses are computed by **rolling out** a **closed-loop control policy `œÄ`** for each target vehicle."  - **`game theoretic`** prediction.
    - **`game theoretic`** prediction.
      - > "Agents are modeled as **_looking ahead_** to consider the **possible ramifications** of their `actions`. This notion of **_looking ahead_** makes **game-theoretic** prediction models more deeply `interaction-aware` than **forward simulation** models based on **_reactive_ closed-loop** control."

</details>

---

**`"Motion Prediction using Trajectory Sets and Self-Driving Domain Knowledge"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2006.04767)]**
**[**:car: `nuTonomy`**]**

- **[** _`multimodal`, `probabilistic`, `mode collapse`, `domain knowledge`, `classification`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2006.04767).](../media/2020_boulton_1.PNG "[Source](https://arxiv.org/abs/2006.04767).")  |
|:--:|
| *Top: The idea of `CoverNet` is to first **generate feasible future trajectories**, and then **classify** them. It uses the past `states` of all road users and a `HD map` to compute a **distribution** over a vehicle's possible future states. Bottom-left: the set of trajectories can be **reduced by considering the current `state` and the dynamics**: at `high speeds`, **sharp turns are not dynamically feasible** for instance. Bottom-right: the contribution here also deals with **"feasibility"**, i.e. tries to reduce the set using **domain knowledge**. A second `loss` is introduced to **penalize predictions that go `off-road`**. The first `loss` (`cross entropy` with closest prediction treated as ground truth) is also adapted: instead of a **`delta` distribution** over the **closest mode**, there is also **probability** assigned to near misses. [Source](https://arxiv.org/abs/2006.04767).* |

Authors: Boulton, F. A., Grigore, E. C., & Wolff, E. M.

- Related work: **[`CoverNet`: Multimodal Behavior Prediction using Trajectory Sets](https://arxiv.org/pdf/1911.10298.pdf)**, (Phan-Minh, Grigore, Boulton, Beijbom, & Wolff, 2019).

- Motivation:
  - Extend their `CoverNet` by including further **"domain knowledge"**.
    - > "Both **dynamic constraints** and **_"rules-of-the-road"_** place strong priors on **likely motions**."
    - `CoverNet`: Predicted trajectories should be **consistent** with the **current dynamic state**.
    - This work : Predicted trajectories should **stay on road**.
  - The main idea is to **leverage the `map` information** by adding an **auxiliary loss** that **penalizes off-road predictions**.

- Motivations and ideas of `CoverNet`:
  - `1-` Avoid the issue of **"mode collapse"**.
    - The prediction problem is treated as **classification** over a **_diverse_ set of trajectories**.
    - The **trajectory sets** for `CoverNet` is available on [`nuscenes-devkit` github](https://github.com/nutonomy/nuscenes-devkit).
  - `2-` Ensure a desired level of **coverage** of the `state` space.
    - The larger and the more diverse the **set**, the higher the **coverage**. One can play with the resolution to ensure **coverage guarantees**, while **pruning** of the set improves the efficiency.
  - `3-` **Eliminate dynamically infeasible trajectories**, i.e. introduced **dynamic constraints**.
    - Trajectories that are **not physically possible** are not considered, which limits the **set of reachable states** and improves the **efficiency**.
    - > "We create a **dynamic trajectory** set based on the current `state` by **integrating forward** with our dynamic model over **diverse control sequences**."

- Two **losses**:
  - `1-` Moving beyond **"standard" `cross-entropy` loss** for **classification**.
    - _What is the_ **_ground truth_** _trajectory?_ Obviously, it is **not part of the set**.
    - One solution: designate **the closest one** in the set.
      - > "We utilize **cross-entropy** with positive samples determined by the element in the **`trajectory set` closest to the actual ground truth** in **minimum average** of point-wise Euclidean distances."
      - Issue: This **will penalize the second-closest trajectory** just as much as **the furthest**, since it ignores the **geometric structure** of the trajectory set.
    - Another idea: use a **weighted** cross-entropy loss, where the **`weight`** is a **function of distance to the ground truth**.
      - > "Instead of a **`delta` distribution** over the **closest mode**, there is also **probability** assigned to `near misses`."
      - A **threshold** defines which trajectories are _"close enough"_ to the ground truth.
    - This weighted loss is adapted to favour **mode diversity**:
      - > "We tried an **_"Avoid Nearby"_ weighted cross entropy** loss that assigns weight of `1` to the **closest match**, `0` to all other trajectories within **`2` meters of ground truth**, and `1/|K|` to the rest. We see that we are able to **increase mode diversity** and recover the performance of the baseline loss."
      - > "Our results indicate that losses that are better able to **enforce mode diversity** may lead to improved performance."
  - `2-` Add an **auxiliary loss** for **off-road predictions**.
    - This helps **learn domain knowledge**, i.e. partially encode _"rules-of-the-road"_.
    - > "This auxiliary loss can easily be **pretrained** using only `map` information (e.g., `off-road` area), which **significantly improves performance on small datasets**."

- Related works for **`predictions`** (we want **_multimodal_** and **_probabilistic_** trajectory predictions):
  - **Input**, i.e. **encoding** of the scene:
    - > "State-of-the-art motion prediction algorithms now typically use `CNNs` to learn appropriate features from a **`birds-eye-view` rendering** of the scene (**map** and road users)."
    - **Graph neural networks** (`GNNs`) looks promising to **encode interactions**.
    - Here: A **`BEV` raster `RGB` image** (fixed size) containing **`map` information** and the **past `states`** of all objects.
      - It is inspired by the work of `UBER`: [Multimodal Trajectory Predictions for Autonomous Driving using Deep Convolutional Networks](http://arxiv.org/abs/1809.10732), (Cui et al., 2018).
  - **Output**, i.e. representing the possible future motions:
    - `1-` **Generative** models.
      - They encode choice over multiple actions via **sampling latent variables**.
      - Issue: **multiple trajectory samples** or **`1`-step policy rollouts** (e.g. `R2P2`) are required at inference.
      - Examples: Stochastic policies, `CVAE`s and `GAN`s.
    - `2-` **Regression**.
      - **Unimodal**: predict a single future trajectory. Issue: **unrealistically average** over behaviours, even when predicting **Gaussian uncertainty**.
      - **Multimodal**: **distribution over multiple trajectories**. Issue: suffer from **mode collapse**.
    - `3-` **Classification**.
      - > "We choose **not to learn an uncertainty distribution** over the space. The **density** of our trajectory sets reduces its benefit compared to the case when there are a only a handful of modes."
      - _How to deal with varying number of classes to predict?_ Not clear to me.

- How to solve `mode collapse` in regression?
  - The authors consider [`MultiPath`](https://arxiv.org/abs/1910.05449) by `Waymo` (detailed also in this page) as their baseline.
  - A **set of anchor boxes** can be used, much like in **object detection**:
  - > "This model implements **ordinal regression** by first choosing among a **fixed set of anchors** (computed **_a priori_**) and then **regressing to residuals** from the chosen anchor. This model predicts a **fixed number of trajectories (`modes`)** and their associated **probabilities**."
  - The authors extend `MultiPath` with **dynamically-computed anchors**, based on the agent's current `speed`.
    - Again, it makes no sense to consider anchors that are **not dynamically reachable**.
    - They also found that using one order of magnitude **more ‚Äúanchor‚Äù trajectories** that Waymo (`64`) is beneficial: **better coverage of space** via anchors, leaving the network to **learn smaller residuals**.

- Extensions:
  - As pointed out by this [`KIT` Master Thesis offer](https://www.mrt.kit.edu/download/Extending_CoverNet_CNN-based_method_for_Trajectory_Prediction.pdf), the current state of `CoverNet` only has a **motion model for cars**. Predicting **bicycles and pedestrians'** motions would be a next step.
  - **Interactions** are ignored now.

</details>

---

**`"PnPNet: End-to-End Perception and Prediction with Tracking in the Loop"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2005.14711)]**
**[** :mortar_board: `University of Toronto` **]**
**[**:car: `Uber`**]**

- **[** _`joint perception + prediction`, `multi-object tracking`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2005.14711).](../media/2020_liang_1.PNG "[Source](https://arxiv.org/abs/2005.14711).")  |
|:--:|
| *The authors propose to leverage **`tracking`** for the **joint `perception`+`prediction`** task. [Source](https://arxiv.org/abs/2005.14711).* |

| ![[Source](https://arxiv.org/abs/2005.14711).](../media/2020_liang_2.PNG "[Source](https://arxiv.org/abs/2005.14711).")  |
|:--:|
| *Top: One main idea is to make the `prediction` module directly reuse the scene context captured in the **`perception` features**, and also consider the **past object `tracks`**. Bottom: a second contribution is the use of a `LSTM` as a **sequence model** to learn the **object trajectory representation**. This encoding is jointly used for the `tracking` and `prediction` tasks. [Source](https://arxiv.org/abs/2005.14711).* |

Authors: Liang, M., Yang, B., Zeng, W., Chen, Y., Hu, R., Casas, S., & Urtasun, R.

- Motivations:
  - `1-` Perform **`perception`** and **`prediction`** jointly, with a single neural network.
    - There for it is called **"Perception and Prediction"**: **`PnP`**.
    - The whole model is also said `end-to-end`, because it is `end-to-end` trainable.
      - This constrast with **modular sequential architectures** where both the **perception output** and **map information** is forwarded to an **independent `prediction` module**, for instance in a **bird‚Äôs eye view** (`BEV`) **raster representation**.
  - `2-` Improve `prediction` by leveraging the (past) **temporal** information (**motion history**) contained in **`tracking` results**.
    - In particular, one goal is to **recover** from long-term object **occlusion**.
      - > "While all these [_vanilla `PnP`_] approaches **share the sensor features** for `detection` and `prediction`, they fail to exploit the **rich information** of actors along the **time dimension** [...]. This may cause problems when dealing with **occluded actors** and may **produce temporal inconsistency** in `predictions`."
    - The idea is to include `tracking` **in the loop** to improve **`prediction`** (motion forecasting):
      - > "While the `detection` module processes **sequential sensor data** and generates **object detections** at each time step **independently**, the **`tracking`** module **associates these estimates across time** for better understanding of object states (e.g., **occlusion reasoning**, **trajectory smoothing**), which in turn provides **richer information** for the **`prediction` module** to produce accurate **future trajectories**."
      - > "**Exploiting motion** from **explicit object trajectories** is more accurate than inferring motion from the features computed from the raw sensor data. [_this reduces the prediction error by (`‚àº6%`) in the experiment_]"
    - All modules share computation as there is a **single backbone network**, and the full model can be trained `end-to-end`.
      - > "While previous joint `perception` and `prediction` models make the prediction module **another convolutional header** on top of the `detection` backbone network, which shares the same features with the `detection` header, in `PnPNet` we put the **`prediction`** module **after explicit object `tracking`**, with the **object trajectory representation** as input."

- _How to_ **_represent_** _(long-term)_ **_trajectories_**_?_
  - The idea is to capture both **`sensor` observation** and **`motion` information** of actors.
  - > "For each object we first extract its **inferred `motion`** (from past detection estimates) and raw observations (from **`sensor` features**) at each time step, and then **model its dynamics** using a recurrent network."
  - > [_interesting choice_] "For **angular velocity** of ego car we parameterize it as its `cosine` and `sine` values."
  - This **trajectory representation** is utilized in both `tracking` and `prediction` modules.

- About **multi-object `tracking`** (`MOT`):
  - There exist two distinct challenges:
    - `1-` The **_discrete_** problem of **`data association`** between previous tracks and current detections.
      - **Association errors** (i.e., **`identity switches`**) are prone to **accumulate** through time.
      - > "The `association problem` is formulated as a **`bipartite` matching problem** so that exclusive **`track-to-detection` correspondence** is guaranteed. [...] Solved with the **`Hungarian algorithm`**."
      - > "Many frameworks have been proposed to solve the `data association problem`: e.g., **Markov Decision Processes** (`MDP`), `min-cost flow`, `linear assignment` problem and `graph cut`."
    - `2-` The **_continuous_** problem of **`trajectory estimation`**.
      - In the proposed approach, the **`LSTM` representation** of associated **new tracks** are refined to generate **smoother trajectories**:
        - > "For **`trajectory refinement`**, since it **reduces the localization error** of online generated perception results, it helps establish a **smoother** and more accurate motion history."
  - The proposed **multi-object tracker** solves both problems, therefore it is said "`discrete`-`continuous`".

</details>

---

**`"VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2005.04259)]**
**[[:memo:](https://blog.waymo.com/2020/05/vectornet.html)]**
**[[üéûÔ∏è](https://youtu.be/BV4EXwlb3yo?t=923)]**
**[**:car: `Waymo`**]**

- **[** _`GNN`, `vectorized representation`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2005.04259).](../media/2020_gao_1.gif "[Source](https://arxiv.org/abs/2005.04259).")  |
|:--:|
| *Both `map features` and our `sensor input` can be **simplified into either a `point`, a `polygon`, or a `curve`**, which can be approximately represented as **polylines**, and eventually further split into vector fragments. The set of such vectors form a simplified **abstracted world** used to make prediction with **less computation** than **rasterized images** encoded with `ConvNets`. [Source](https://arxiv.org/abs/2005.04259).* |

| ![[Source](https://arxiv.org/abs/2005.04259).](../media/2020_gao_1.PNG "[Source](https://arxiv.org/abs/2005.04259).")  |
|:--:|
| *A **vectorized** representation of the scene is preferred to the combination (**rasterized** rendering + **`ConvNet`** encoding). A **global interaction graph** can be built from these **vectorized** elements, to model the **higher-order relationships** between entities. To further improve the prediction performance, a **supervision** auxiliary task is introduced. [Source](https://arxiv.org/abs/2005.04259).* |

Authors: Gao, J., Sun, C., Zhao, H., Shen, Y., Anguelov, D., Li, C., & Schmid, C.

- Motivations:
  - `1-` **Reduce computation cost** while offering good prediction performances.
  - `2-` Capture **long range context information**, for longer horizon prediction.
    - `ConvNets` are commonly used to encode the scene context, but they have **limited receptive field**.
    - And increasing `kernel size` and `input image size` is not so easy:
      - **FLOPs** of `ConvNets` increase **quadratically** with the `kernel size` and `input image size`.
      - The **number of parameters** increases **quadratically** with the `kernel size`.
  - Four ingredients:
    - `1-` A **vectorized** representation is preferred to the combination (**rasterized** rendering + **`ConvNet`** encoding).
    - `2-` A **graph network**, to model **interactions**.
    - `3-` **Hierarchy**, to first encode `map` and `sensor` information, and then learn interactions.
    - `4-` A **supervision** auxiliary task, in parallel to the prediction task.
- Two kind of **input**:
  - `1-` **`HD map`** information: **Structured road context information** such as `lane boundaries`, `stop/yield signs`, `crosswalks` and `speed bumps`.
  - `2-` **`Sensor`** information: Agent **trajectories**.
- _How to_ **_encode the scene context_** _information?_
  - `1-` **Rasterized representation**.
    - `Rendering`: in a **bird-eye image**, with colour-coded attributes. Issue: colouring requires manual specifications.
    - `Encoding`: encode the scene context information with **`ConvNets`**. Issue: **receptive field** may be limited.
    - > "The most popular way to **incorporate highly detailed maps** into behavior prediction models is by **rendering the map into pixels** and **encoding the scene information**, such as `traffic signs`, `lanes`, and `road boundaries`, with a **convolutional neural network** (`CNN`). However, this process requires a **lot of compute and time**. Additionally, processing maps as imagery makes it challenging to **model long-range geometry**, such as _lanes merging ahead_, which affects the quality of the predictions."
    - Impacting parameters:
      - Convolutional **kernel sizes**.
      - **Resolution** of the rasterized images.
      - **Feature cropping**:
        - > "A larger crop size (`3` vs `1`) can significantly improve the performance, and **cropping along observed trajectory** also leads to better performance."
  - `2-` **Vectorized representation**
    - All `map` and `trajectory` elements can be **approximated as sequences of vectors**.
    - > "This avoids **lossy rendering** and **computationally intensive `ConvNet` encoding** steps."
  - About **graph neural networks (`GNN`s)**, from [Rishabh Anand](https://www.notion.so/Rishabh-Anand-ceb23e08fecf4afb8732fe0f55039f90)'s [medium article](https://medium.com/dair-ai/an-illustrated-guide-to-graph-neural-networks-d5564a551783):
    - `1-` Given a graph, we first **convert the `nodes`** to recurrent units and the `edges` to **feed-forward** neural networks.
    - `2-` Then we perform **`Neighbourhood Aggregation`** (**`Message Passing`**) for all nodes `n` number of times.
    - `3-` Then we **sum over the embedding vectors** of all nodes to **get graph representation `H`**. Here the _"Global interaction graph"._
    - `4-` Feel free to pass `H` into higher layers or use it to represent the graph‚Äôs unique properties! _Here to learn interaction models to make_ **_prediction_**.
- About **hierarchy**:
  - `1-`First, **aggregate information** among **vectors** inside a **polyline**, namely `polyline subgraphs`.
    - **Graph neural networks (`GNN`s)**  are used to **incorporate these sets of vectors**
    - > "We treat each vector vi belonging to a polyline Pj as a node in the graph with node features."
    - _How to encode_ **_attributes_** _of these geometric elements?_ E.g. `traffic light state`, `speed limit`?
      - _I must admit I did not fully understand. But from what I read on medium:_
        - Each `node` has a **set of features** defining it.
        - Each `edge` may **connect `nodes` together** that have **similar features**.
  - `2-` Then, model the **higher-order relationships** among **polylines**, directly from their **vectorized form**.
    - Two interactions are jointly modelled:
      - `1-` The **interactions of multiple agents**.
      - `2-` Their interactions with the entities from **road maps**.
        - E.g. a **car enters an intersection**, or a pedestrian approaches a crosswalk.
        - > "We clearly observe that adding map information significantly improves the trajectory prediction performance."
- An **auxiliary task**:
  - `1-` **Randomly masking out `map` features** during training, such as a `stop sign` at a four-way intersection.
  - `2-` Require the net to **complete it**.
  - > "The goal is to incentivize the model to **better capture interactions among nodes**."
  - And to learn to **deal with occlusion**.
  - > "Adding this objective consistently helps with performance, especially at **longer time horizons**".
- Two **training objectives**:
  - `1-` Main task = **Prediction**. **Future trajectories**.
  - `2-` Auxiliary task = **Supervision**. `Huber loss` between **predicted node features** and **ground-truth masked node features**.
- Evaluation metrics:
  - The "widely used" **Average Displacement Error** (`ADE`) computed over the **entire trajectories**.
  - The **Displacement Error** at `t` (`DE@ts`) metric, where `t` in {`1.0`, `2.0`, `3.0`} seconds.
- Performances and computation cost.
  - `VectorNet` is compared to `ConvNets` on the **[`Argoverse`](https://www.argoverse.org/) forecasting dataset**, as well as on some `Waymo` in-house prediction dataset.
  - `ConvNets` consumes **`200+` times more `FLOPs`** than `VectorNet` for a single agent: `10.56G` vs `0.041G`. Factor `5` when there are `50` agents per scene.
  - `VectorNet` needs **`29%` of the parameters** of `ConvNets`: `72K` vs `246K`.
  - `VectorNet` achieves up to `18%` better performance on `Argoverse`.

</details>

---

**`"Online parameter estimation for human driver behavior prediction"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2005.02597)]**
**[[:octocat:](https://github.com/sisl/ngsim_env/tree/idm_pf_NGSIM)]**
**[**:mortar_board: `Stanford`**]**
**[**:car: `Toyota Research Institute`**]**

- **[** _`stochastic IDM`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2005.02597).](../media/2020_bhattacharyya_1.PNG "[Source](https://arxiv.org/abs/2005.02597).")  |
|:--:|
| *The vanilla [IDM](https://arxiv.org/abs/cond-mat/0002177) is a **parametric rule-based car-following model** that balances two forces: the desire to achieve **free speed** if there were no vehicle in front, and the **need to maintain safe separation** with the vehicle in front. It outputs an **acceleration** that is guaranteed to be **collision free**. The **stochastic** version introduces a new model parameter `œÉ-IDM`. [Source](https://arxiv.org/abs/2005.02597).* |

Authors: Bhattacharyya, R., Senanayake, R., Brown, K., & Kochenderfer

- Motivations:
  - `1-` Explicitly **model stochasticity** in the behaviour of individual drivers.
    - Complex **multi-modal** distributions over possible outcomes should be modelled.
  - `2-` Provide **safety guarantees**
  - `3-` **Highway** scenarios: no urban intersection.
  - The methods should combine advantages of `rule-based` and `learning-based` estimation/prediction methods:
    - **Interpretability**.
    - **Guarantees on safety** (the learning-based model **Generative Adversarial Imitation Learning ([**`GAIL`**](https://arxiv.org/abs/1606.03476))** used as baseline is not collision free).
    - Validity even in regions of the state space that are under-represented in the data.
    - High **expressive power** to capture **nuanced driving behaviour**.
- About the method:
  - > "We apply **online parameter estimation** to an extension of the **Intelligent Driver Model** [IDM](https://arxiv.org/abs/cond-mat/0002177) that explicitly models stochasticity in the behavior of individual drivers."
  - This rule-based method is **online**, as opposed for instance to the IDM with parameters obtained by **offline estimation**, using **non-linear least squares**.
  - **Particle filtering** is used for the recursive Bayesian estimation.
  - The derived parameter estimates are then used for **forward motion prediction**.
- About the **estimated parameters** (per observed vehicle):
  - `1-` The **desired velocity** (`v-des`).
  - `2-` The driver-dependent **stochasticity on acceleration** (`œÉ-IDM`).
  - They are assumed **stationary** for each driver, i.e., human drivers **do not change their latent driving behaviour** over the time horizons.
- About the **datasets**:
  - `NGSIM` for [US Highway 101](https://www.fhwa.dot.gov/publications/research/operations/07030/index.cfm) at `10 Hz`.
  - Highway Drone Dataset ([`HighD`](https://arxiv.org/ftp/arxiv/papers/1810/1810.05642.pdf)) at `25 Hz`.
  - `RMSE` of the **position** and **velocity** are used to measure ‚Äúcloseness‚Äù of a **predicted trajectory** to the corresponding ground-truth trajectory.
  - **Undesirable events**, e.g. _collision_, _going off-the-road_, _hard braking_, that occur in each scene prediction are also considered.
- How to deal with the **_"particle deprivation problem"_**?:
  - Particle deprivation = particles **converge to one region** of the state space and there is **no exploration** of other regions.
  - `Dithering` method = **external noise** is added to **aid exploration** of state space regions.
  - From (Sch√∂n, Gustafsson, & Karlsson, 2009) in ["The Particle Filter in Practice"](http://user.it.uu.se/~thosc112/pubpdf/schongk2011.pdf):
    - > "Both the **`process`** noise and **`measurement` noise** distributions need some **dithering** (**increased covariance**).  Dithering the `process` noise is a well-known method to mitigate the **sample impoverishment problem**.  Dithering the `measurement` noise is a good way to mitigate the effects of **outliers** and to robustify the `PF` in general".
  - Here:
    - > "We implement dithering by **adding random noise** to the top `20%` particles ranked according to the corresponding likelihood. The noise is sampled from a **discrete uniform distribution** with `v-des` `‚àà` {`‚àí0.5`, `0`, `0.5`} and `œÉ-IDM` `‚àà` {`‚àí0.1`, `0`, `0.1`}. (This **preserves the discretization** present in the initial sampling of particles).
- Future works:
  - **Non-stationarity**.
  - Combination with a **lane changing model** such as [`MOBIL`](https://mtreiber.de/publications/MOBIL_TRB.pdf) to extend to two-dimensional driving behaviour.

</details>

---

**`"PLOP: Probabilistic poLynomial Objects trajectory Planning for autonomous driving"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2003.08744)]**
**[[üéûÔ∏è](TO COME)]**
**[** :car: `Valeo` **]**

- **[** _`Gaussian mixture`, `multi-trajectory prediction`, [`nuScenes`](https://arxiv.org/abs/1903.11027), [`A2D2`](https://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html), `auxiliary loss`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2003.08744).](../media/2020_buhet_1.PNG "[Source](https://arxiv.org/abs/2003.08744).")  |
|:--:|
| *The architecture has two main sections: an **`encoder`** to synthesize information and the **`predictor`** where we exploit it. Note that `PLOP` does not use the **classic `RNN` decoder scheme** for trajectory generation, preferring a **single step** version which predicts the **coefficients of a polynomial function** instead of the **consecutive points**. Also note the **`navigation command`** that conditions the ego prediction. [Source](https://arxiv.org/abs/2003.08744).* |

| ![[Source](https://arxiv.org/abs/2003.08744).](../media/2020_buhet_2.PNG "[Source](https://arxiv.org/abs/2003.08744).")  |
|:--:|
| *`PLOP` uses **multimodal sensor** data input: **`Lidar` and `camera`**. The map is accumulated over the **past `2s`**, so `20` frames. It produces a **multivariate gaussian mixture** for a **fixed number of `K`** possible trajectories over a **`4s` horizon**. **Uncertainty** and **variability** are handled by predicting vehicle trajectories as a **probabilistic Gaussian Mixture models**, constrained by a **polynomial** formulation. [Source](https://arxiv.org/abs/2003.08744).* |

Authors: Buhet, T., Wirbel, E., & Perrotton, X.

- Motivations:
  - The goal is to predicte **multiple** feasible future trajectories **both** for the ego vehicle and neighbors through a **probabilistic** framework.
    - In addition in an **`end-to-end` trainable** fashion.
  - It builds on a previous work: ["Conditional vehicle trajectories prediction in carla urban environment"](https://arxiv.org/abs/1909.00792) - (Buhet, Wirbel, & Perrotton, 2019). _See analysis further below._
    - The trajectory prediction based on **polynomial** representation is upgraded from **deterministic output** to **multimodal probabilistic output**.
    - It re-uses the **navigation command** input for the **conditional part** of the network, e.g. `follow`, `left`, `straight`, `right`.
    - One main difference is the introduction of a new input sensor: **Lidar**.
    - And adding a **semantic segmentation auxiliary loss**.
  - The authors also reflect about _what_ **_metrics_** _is relevant for trajectory prediction_:
    - > "We suggest to use two additional criteria to evaluate the predictions errors, one based on the **most confident prediction**, and one **weighted by the confidence** [_how alternative trajectories with non maximum weights compare to the most confident trajectory_]."
- One term: **"Probabilistic poLynomial Objects trajectory Planning"** = **`PLOP`**.

- I especially like their **review on related works** about data-driven predictions (section taken from the paper):
  - **[`SocialLSTM`](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf)**: encodes the relations between close agents introducing a **social pooling layer**.
  - -**Deterministic** approaches derived from `SocialLSTM`:
    - [`SEQ2SEQ`](https://arxiv.org/abs/1802.06338) presents a new **`LSTM`-based encoder-decoder** network to predict trajectories into an **occupancy grid map**.
    - [`SocialGAN`](https://arxiv.org/abs/1803.10892) and [`SoPhie`](https://arxiv.org/abs/1806.01482) use **generative adversarial networks** to tackle uncertainty in future paths and augment the original set of samples.
      - `CS-LSTM` extends `SocialLSTM` using **convolutional layers** to encode the **relations between the different agents**.
    - `ChauffeurNet` uses a sophisticated neural network with a **complex high level scene representation** (`roadmap`, `traffic lights`, `speed limit`, `route`, `dynamic bounding boxes`, etc.) for **deterministic** ego vehicle trajectory prediction.
  - Other works use a **_graph representation_** of the **interactions between the agents** in combination with **neural networks** for trajectory planning.
  - **Probabilistic** approaches:
    - Many works like **[`PRECOG`](https://arxiv.org/abs/1905.01296)**, [`R2P2`](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nicholas_Rhinehart_R2P2_A_ReparameteRized_ECCV_2018_paper.pdf), [`Multiple Futures Prediction`](https://arxiv.org/abs/1911.00997), `SocialGAN` include probabilistic estimation by adding a **probabilistic framework at the end** of their architecture producing **multiple trajectories** for ego vehicle, nearby vehicles or both.
    - In **[`PRECOG`](https://arxiv.org/abs/1905.01296)**, `Rhinehart et al.` build a **probabilistic model** that **explicitly models interactions** between agents, using **latent variables** to model the **plausible reactions** of agents to each other, with a possibility to **pre-condition the trajectory** of the ego vehicle by **a goal**.
    - `MultiPath` also reuses an idea from **object detection** algorithms using **trajectory anchors** extracted from the training data for ego vehicle prediction.

- About the **auxiliary** semantic segmentation task.
  - Teaching the network to **represent such semantic** in its features improves the prediction.
  - > "Our objective here is to make sure that in the `RGB` image **encoding**, there is **information about the road position and availability**, the applicability of the **traffic rules** (traffic sign/signal), the **vulnerable road users** (pedestrians, cyclists, etc.) position, etc. This information is useful for **trajectory planning** and brings some **explainability** to our model."

- About **interactions** with other vehicles.
  - The prediction for each vehicle does **not have direct access** to the sequence of history positions of others.
  - > "The encoding of the interaction between vehicles is implicitly computed by the **birdview encoding**."
  - The number of predicted trajectories is **fixed** in the network architecture. **`K=12`** is chosen.
    - > "It allows our architecture to be **agnostic to the number** of considered neighbors."

- **Multi-trajectory** prediction in a **probabilistic** framework.
  - > "We want to predict a **fixed number `K`** of possible trajectories for each vehicle, and associate them to a **probability distribution** over `x` and `y`:  `x` is the **longitudinal** axis, `y` the **lateral** axis, pointing left."
  - About the **Gaussian Mixture**.
    - Vehicle trajectories are predicted as **probabilistic Gaussian Mixture models**, constrained by a **polynomial formulation**: The **mean** of the distribution is expressed using a **polynomial** of **degree `4` of time**.
    - > "In the end, this representation can be interpreted as **predicting `K` trajectories**, each associated with a **confidence `œÄk`** [_mixture weights shared for all sampled points belonging to the same trajectory_], with **sampled points** following a **Gaussian distribution** centered on (`¬µk,x,t`, `¬µk,y,t`) and with **standard deviation** (`œÉk,x,t`, `œÉk,y,t`)."
    - > "`PLOP` does not use the **classic `RNN` decoder** scheme for trajectory generation, preferring a **single step version** which predicts the **coefficients of a polynomial function** instead of the **consecutive points**."
    - This offers a measure of **uncertainty** on the predictions.
    - For the ego car, the probability distribution is **conditioned by the navigation command**.
  - About the **loss**:
    - `negative log-likelihood` over **all sampled points** of the **ground truth** ego and neighbour vehicles trajectories.
    - There is also the **auxiliary** `cross entropy loss` for segmentation.

- Some findings:
  - The presented model seems very robust to the **varying number of neighbours**.
    - Finally, for **`5` agents or more**, `PLOT` outperforms by a large margin all `ESP` and `PRECOG`, on authors-defined metrics.
    - > "This result might be explained by our **interaction encoding** which is robust to the variations of `N` using only **multiple birdview projections** and our **non-iterative single step** trajectory generation."
  - > "Using `K = 1` approach yields very **poor results**, also visible in the training loss. It was an anticipated outcome due to the **ambiguity of human behavior**."

</details>

---

**`"Probabilistic Future Prediction for Video Scene Understanding"`**

- **[** `2020` **]**
**[[:memo:](https://arxiv.org/abs/2003.06409)]**
**[[üéûÔ∏è](https://www.youtube.com/watch?v=EwEfs2R4RIA)]**
**[[üéûÔ∏è](https://wayve.ai/blog/predicting-the-future) (blog)]**
**[** :mortar_board: `University of Cambridge` **]**
**[** :car: `Wayve` **]**

- **[** _`multi frame`, `multi future`, `auxiliary learning`, `multi-task`, `conditional imitation learning`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/2003.06409).](../media/2020_hu_1.PNG "[Source](https://arxiv.org/abs/2003.06409).")  |
|:--:|
| *One main **motivation** is to supply the **`Control`** module (e.g. policy learnt via `IL`) with a **representation** capable of **modelling probability of future events**. The `Dynamics` module produces such **spatio-temporal representation**, not directly from images but from learnt **scene features**. That embeddings, that are used by the `Control` in order to learn driving policy, can be explicitly decoded to future `semantic segmentation`, `depth`, and `optical flow`. Note that the **stochasticity of the future** is modelled with a conditional variational approach minimises the divergence between the **`present distribution`** (what could happen given what we have seen) and the **`future distribution`** (what we observe actually happens). During inference, **diverse futures** are generated by sampling from the `present distribution`. [Source](https://arxiv.org/abs/2003.06409).* |

| ![[Source](https://wayve.ai/blog/predicting-the-future).](../media/2020_hu_1.gif "[Source](https://wayve.ai/blog/predicting-the-future).")  |
|:--:|
| *There are **many possible futures** approaching this four-way intersection. Using **`3` different noise vectors** makes the model imagine **different driving manoeuvres** at an intersection: `driving straight`, `turning left` or `turning right`. These samples predict `10` frames, or **`2` seconds** into the future. [Source](https://wayve.ai/blog/predicting-the-future).* |

| ![[Source](https://www.youtube.com/watch?v=EwEfs2R4RIA).](../media/2020_hu_2.gif "[Source](https://www.youtube.com/watch?v=EwEfs2R4RIA).")  |
|:--:|
| *The **differential entropy** of the **present distribution**, characterizing **how unsure the model is about the future** is used. As we approach the intersection, it increases. [Source](https://www.youtube.com/watch?v=EwEfs2R4RIA).* |

Authors: Hu, A., Cotter, F., Mohan, N., Gurau, C., & Kendall, A.

- Motivations:
  - `1-` Supply the **control** module (e.g. `IL`) with an appropriate **representation for `interaction-aware` and `uncertainty-aware` decision-making**, i.e. one capable of **modelling probability of future events**.
    - Therefore the policy should receive temporal **features explicitly trained to predict the future**.
      - Motivation for that: It is difficult to learn an **effective temporal** representation by only using **imitation error** as a learning signal.
  - Others:
  - `2-` **_"`multi frame` and `multi future`_"** prediction.
    - Perform prediction:
      - ... based on **multiple past frames** (i.e. not a single one).
      - ... and producing **multiple possible outcomes** (i.e. not deterministic).
        - **Predict the stochasticity** of the future, i.e. contemplate **multiple possible outcomes** and estimate the **multi-modal uncertainty**.
  - `3-` Offer a **differentiable / end-to-end trainable** system, as opposed to system that reason over **hand-coded representations**.
    - I understand it as **considering the loss of the `IL` part** into the layers that create the **latent representation**.
  - `4-` Cope with **multi-agent interaction** situations such as traffic merging, i.e. do not predict the behaviour of each actor in the scene **independently**.
    - For instance by **jointly predicting** ego-motion and motion of other dynamic agents.
  - `5-` Do not rely on any `HD-map` to predict the static scene, to stay **resilient to `HD-map` errors** due to e.g. roadworks.

- **`auxiliary learning`**: The loss used to train the **latent representation** is composed of **three terms** (c.f. motivation `3-`):
  - **`future-prediction`**: weighted sum of future `segmentation`, `depth` and `optical flow` losses.
  - **`probabilistic`**: `KL`-divergence between the `present` and the `future` distributions.
  - **`control`**: **regression** for future time-steps up to some `Future control horizon`.
- Let's explore some ideas behinds these **three components**.

- `1-` **Temporal video encoding**: _How to build a_ **_temporal_** _and_ **_visual_** _representation?_
  - _What should be predicted?_
    - > "Previous work on probabilistic future prediction focused on **trajectory forecasting** [DESIRE, Lee et al. 2017, Bhattacharyya et al. 2018, PRECOG, Rhinehart et al. 2019] or were restricted to **single-frame image generation** and **low resolution** (64x64) datasets that are either **simulated** (Moving MNIST) or with static scenes and limited dynamics."
    - > "Directly predicting in the **high-dimensional space of image pixels** is unnecessary, as some details about the appearance of the world are **irrelevant for planning and control**."
    - Instead, the task is to predict a more complete **scene representation** with `segmentation`, `depth`, and `flow`, **two seconds** in the future.

  - _What should the `temporal module` process?_
    - The **temporal model** should learn the **spatio-temporal features** from **perception encodings** [as opposed to RGB images].
    - These encodings are **"scene features"** extracted from images by a `Perception` module. They constitute a **more powerful** and **compact representation** compared to RGB images.

  - _How does the `temporal module` look like?_
    - > "We propose a new **spatio-temporal architecture** that can learn **hierarchically** more complex features with a **novel 3D convolutional** structure incorporating both **_local_** and **_global_** **space and time context**."
  - The authors introduce a so-called **`Temporal Block`** module for **temporal video encoding**.
    - These `Temporal Block` should help to learn **hierarchically more complex temporal features**. With two main ideas:
    - `1-` Decompose the **convolutional filters** and play with all possible configuration.
      - > "Learning **`3D` filters** is **hard**. Decomposing into **two subtasks** helps the network learn more efficient."
      - > "State-of-the-art works decompose `3D` filters into **_spatial_** and **_temporal_** convolutions. The model we propose **further breaks down** convolutions into **many space-time combinations** and **context aggregation modules**, stacking them together in a more **complex hierarchical representation**."
    - `2-` Incorporate the **_"global context"_** in the features (_I did not fully understand that_).
      - They concatenate some **local features** based on `1x1x1` compression with some **global features** extracted with `average pooling`.
      - > "By **pooling** the features spatially and temporally at **different scales**, each individual feature map also has information about the **global scene context**, which helps in ambiguous situations."

- `2-` **Probabilistic prediction**: _how to generate_ **_multiple futures_**_?_
  - > "There are various reasons why **modelling the future** is incredibly **difficult**: natural-scene data is **rich in details**, most of which are **irrelevant** for the driving task, dynamic agents have **complex temporal dynamics**, often controlled by **unobservable variables**, and the future is **inherently uncertain**, as **multiple futures** might arise from a unique and deterministic past."
  - The idea is that the **uncertainty of the future** can be estimated by making the prediction **_probabilistic_**.
    - > "From a unique past in the real-world, **many futures are possible**, but in reality **we only observe one future**. Consequently, modelling **multi-modal futures from deterministic** video training data is extremely challenging."
    - Another challenge when trying to learn a **multi-modal prediction** model:
      - > "If the network predicts **a plausible future**, but one that **did not match the given training sequence**, it will be heavily penalised."
    - > "Our work addresses this by encoding the **future state** into a low-dimensional **`future distribution`**. We then allow the model to have a **privileged view of the future** through the future distribution at **_training_ time**. As we cannot use the future at **_test_ time**, we train a **`present distribution`** (using only the current state) to **match the `future distribution`** through a `KL`-divergence loss. We can then **sample from the present distribution** during inference, when we do not have access to the future."
  - To put it another way, **two probability distributions** are modelled, in a **_conditional variational_** approach:
    - A **present distribution `P`**, that represents all **_what could happen_** given the past context.
    - A **future distribution `F`**, that represents **_what actually happened_** in that particular observation.
  - > [Learning to **align** the `present distribution` with the `future distribution`] "As the **future is multimodal**, different futures might arise from a unique past context `zt`. Each of these futures will be captured by the **future distribution `F`** that will pull the **present distribution `P`** towards it."
  - _How to evaluate predictions?_
    - > "Our probabilistic model should be **accurate**, that is to say at least one of the generated future should match the ground truth future. It should also **be diverse**".
    - The authors use a **diversity distance metric (`DDM`)**, which measures both **accuracy** and **diversity** of the distribution.
  - _How to quantify uncertainty?_
    - The framework can automatically infer which scenes are **unusual or unexpected** and where the model is **uncertain of the future**, by computing the **differential entropy** of the `present distribution`.
    - This is useful for **understanding edge-cases** and when the model needs to **"pay more attention"**.

- `3-` The rich **spatio-temporal features** explicitly trained to **predict the future** are used to learn a **driving policy**.
  - **Conditional Imitation Learning** is used to learn `speed` and `sterring` controls, i.e. **regressing** to the expert's true control actions {`v`, `Œ∏`}.
    - One reason is that it is **immediately transferable** to the **real world**.
  - From the **ablation study**, it seems to highly benefit from both:
    - `1-` The **`temporal features`**.
      > "It is too difficult to forecast how the future is going to evolve **with a single image**".
    - `2-` The fact that these features are capable of **`probabilistic predictions`**.
      - Especially for **multi-agent interaction** scenarios.

  - About the **training set**:
    - > "We address the **inherent dataset bias** by sampling data **uniformly across lateral and longitudinal dimensions**. First, the data is split into a **histogram of bins** by `steering`, and subsequently by `speed`. We found that **weighting** each data point **proportionally to the width of the bin** it belongs to avoids the need for alternative approaches such as **data augmentation**."

- One exciting future direction:
  - For the moment, the `control` module takes the **representation learned from dynamics models**. And **ignores the predictions** themselves.
    - _By the way, why are predictions, especially for the ego trajectories, not conditionned on possible actions?_
  - It could use these **probabilistic embedding** capable of predicting multi-modal and plausible futures to **generate imagined experience** to train a policy in a model-based `RL`.
  - The design of the **`reward` function** from the **latent space** looks challenging at first sight.

</details>

---

**`"Efficient Behavior-aware Control of Automated Vehicles at Crosswalks using Minimal Information Pedestrian Prediction Model"`**

- **[** `2020` **]**
**[[:memo:](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model)]**
**[** :mortar_board: `University of Michigan`, `University of Massachusetts` **]**

- **[** _`interaction-aware decision-making`, `probabilistic hybrid automaton`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).](../media/2020_jayaraman_3.PNG "[Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).")  |
|:--:|
| *The **pedestrian crossing behaviour** is modelled as a **probabilistic hybrid automaton**. [Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).* |

| ![[Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).](../media/2020_jayaraman_2.PNG "[Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).")  |
|:--:|
| *The **interaction** is captured inside a **gap-acceptance model**: the pedestrian evaluates the **available time gap to cross the street** and either **accept the gap** by starting to cross or **reject the gap by waiting** at the crosswalk. [Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).* |

| ![[Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).](../media/2020_jayaraman_1.PNG "[Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).")  |
|:--:|
| *The **baseline controller** used for comparison is a **finite state machine** (`FSM`) with four states. Whenever a pedestrian **starts walking to cross the road**, the controller always tries to stop, either by `yielding` or through `hard stop`. [Source](https://www.researchgate.net/publication/339784634_Efficient_Behavior-aware_Control_of_Automated_Vehicles_at_Crosswalks_using_Minimal_Information_Pedestrian_Prediction_Model).* |

Authors: Jayaraman, S. K., Jr, L. P. R., Yang, X. J., Pradhan, A. K., & Tilbury, D. M.

- Motivations:
  - Scenario: interaction with a pedestrian `approaching`/`crossing`/`waiting` at a **crosswalk**.
  - `1-` A (`1.1`) **simple** and (`1.2`) **interaction-aware** pedestrian **`prediction` model**.
    - That means no requirement of extensive amounts of data.
    - >  "The **crossing model** as a **hybrid system** with a **gap acceptance model** that required **minimal information**, namely pedestrian's `position` and `velocity`".
      - It **does not require information** about pedestrian `actions` or `pose`.
      - It builds on ["Analysis and prediction of pedestrian crosswalk behavior during automated vehicle interactions"](https://deepblue.lib.umich.edu/handle/2027.42/154053) by (Jayaraman, Tilbury, Yang, Pradhan, & Jr, 2020).
  - `2-` Effectively incorporating these **`predictions`** in a **`control`** framework
    - The idea is to first **forecast the position** of the pedestrian using a pedestrian model, and then **react accordingly**.
  - `3-` Be efficient on both `waiting` and `approaching` pedestrian scenarios.
    - Assuming always a `crossing` may lead to **over-conservative policies**.
    - > "[in simulation] only a fraction of pedestrians (`80%`) are **randomly assigned the intention** to **cross the street**."

- _Why are **`CV`** and **`CA`** **prediction models** not applicable?_
  - > "At crosswalks, pedestrian behavior is **much more unpredictable** as they have to wait for an opportunity and decide when to cross."
  - **Longer durations** are needed.
    - `1-` **Interaction** must be taken into account.
    - `2-` The authors decide to **model pedestrians** as a **hybrid automaton** that **switches between discrete actions**.
- One term: **Behavior-aware Model Predictive Controller** (**`B-MPC`**)
  - `1-` The pedestrian **crossing behaviour** is modelled as a **probabilistic hybrid automaton**:
    - Four **states**: `Approach Crosswalk`, `Wait`, `Cross`, `Walk away`.
    - Probabilistic **transitions**: using **pedestrian's `gap acceptance`** - hence **capturing interactions**.
      - > "_What is the probability of accepting the current traffic gap?_
      - > "Pedestrians evaluate the **available time gap to cross the street** and either **accept the gap** by starting to cross or **reject the gap by waiting** at the crosswalk."
  - `2-` The problem is formulated as a **constrained quadratic optimization** problem:
    - Cost: `success` (passing the crosswalk), `comfort` (penalize jerk and sudden changes in acceleration), `efficiency` (deviation from the reference speed).
    - Constraints: respect `motion model`, restrict `velocity`, `acceleration`, as well as `jerk`, and ensure **`collision avoidance`**.
    - Solver: standard **quadratic program solver** in `MATLAB`.
- Performances:
  - **Baseline controller**:
    - **Finite state machine** (`FSM`) with four states: `Maintain Speed`, `Accelerate`, `Yield`, and `Hard Stop`.
    - > "Whenever a pedestrian **starts walking to cross the road**, the controller always tries to stop, either by _`yielding`_ or through _`hard stop`_."
    - > "The **Boolean variable `InCW`**, denotes the pedestrian‚Äôs crossing activity: `InCW=1` from the time the pedestrian **started moving laterally** to cross until they completely crossed the `AV` lane, and `InCW=0` otherwise."
    - That means the baseline controller **does not react** at all to "non-crossing" cases since it never sees the pedestrian crossing laterally.
  - > "It can be seen that the **`B-MPC` is more aggressive, efficient, and comfortable** than the baseline as observed through the **higher average velocity**, lower average acceleration effort, and lower average jerk respectively."

</details>

---

**`"Human Motion Trajectory Prediction: A Survey"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1905.06113)]**
**[** :mortar_board: `√ñrebro University`, `CMU`, `TU Delft` **]**
**[** :car: `Bosch` **]**

- **[** _`survey`, `physics-based`, `pattern-based`, `planning-based`, `manoeuvre recognition`, `goal-informed`, `context-aware`, `contextual cues`, `social force`_ **]**

<details>
<summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/1905.06113).](../media/2019_rudenko_3.png "[Source](https://arxiv.org/abs/1905.06113).") |
|:--:|
| *The focus is on **human** motion prediction: not on **cars**. Approaches differ in the way they **represent**, parametrize, (learn) and solve the task. Modelling approaches can be divided into **three families**: `physics`-based, `pattern`-based, `planning`-based. [Source](https://arxiv.org/abs/1905.06113).* |

| ![[Source](https://arxiv.org/abs/1905.06113).](../media/2019_rudenko_2.png "[Source](https://arxiv.org/abs/1905.06113).") |
|:--:|
| *What information to consider? All modelling approaches can be extended with so-called **contextual cues** of the **target agent**, `static` and `dynamic` environment. [Source](https://arxiv.org/abs/1905.06113).* |

| ![[Source](https://arxiv.org/abs/1905.06113).](../media/2019_rudenko_1.png "[Source](https://arxiv.org/abs/1905.06113).") |
|:--:|
| *Some **motion trajectories** datasets. Mainly for **pedestrians**. [Source](https://arxiv.org/abs/1905.06113).* |

Authors: Rudenko, A., Palmieri, L., Herman, M., Kitani, K. M., Gavrila, D. M., & Arras, K. O.

- About **`pedestrian`** (_not `vehicle`_) **motion prediction** in `AD`:
  - > "Most works consider the scenario of the **laterally crossing pedestrian**, dealing with the question what the latter will do at the **curbside**: `start walking`, `continue walking`, or `stop walking`."
  - > "It is difficult to **compare the experimental results**, as the **datasets are varying** (different `timings` of same scenario, different `sensors`, different `metrics`)."

- Prediction **methods** can be classified in three large categories:
  - `1-` **`physics`-based**: **reactive** `sense`-`predict`.
  - `2-` **`pattern`-based**: `sense`-**`learn`**-`predict`.
  - `3-` **`planning`-based**: `sense`-**`reason`**-`predict`.
    - Agents **reason about `intentions`** and possible **ways to the `goal`**.

  - These categories do not form a **partition**. Example of **combinations**:
    - `planning`-based using `physics`-based **`transition` functions**.
    - `physics`-based methods **tuned** with **learned parameters**.
    - `planning`-based approaches using `IRL` (`learning` methods) to recover the hidden `reward` function.
    - In general, `combinations` hold a great potential, e.g. use **disagreements** to detect **uncertainty**.

- `1-` **`Physics`-based** methods.
  - Suitable in those situations where:
    - The **effect** of other agents or the static environment **can be modelled** as **`forces` acting** upon an agent.
    - The **agent's motion `dynamics`** can be **modelled by an _explicit_ `transition`** function.
  - Limitations:
    - They might not **capture well the complexity** of the real world.
    - Mainly valid for **short prediction** horizons.
    - Potential burden of **parameter tuning**.
  - Advantages:
    - Applicability **across multiple domains** under mild conditions.
      > "`physics`-based approaches can be readily applied **across multiple environments**, without the need for **training datasets** (some data for **parameter estimation** is useful, though)."
    - **Interpretability**.
    - Usually **simple** and fast approximate inference.

  - Idea:
    - > "Motion is predicted by **forward simulating** a set of **_explicitly defined_ `dynamics` equations** that follow a **`physics` inspired model**."
    - In **`1`-step ahead** predictions with **first-order `Markov`** assumption:
      - A **`transition` function** (or `dynamics` model) **`f`** is defined. With some `dt`.
      - `next_state` = `f`(`state`, `action`) + `noise`.
      - The task is to estimate (`sense`) `state` and `action`. Inference is made from various **estimated or observed `cues`**.
      - Extension to consider not just the **current `state`** (first-order `Markov`), but an **history of `states`**:
        - > "The early work by `Zhu (1991)` uses an **autoregressive moving average model** as `transition` function of a **Hidden Markov Model (`HMM`)** to predict **occupancy probabilities** of moving obstacles **over multiple time steps** with applications to predictive planning."
    - **What to do with this `f`?**
      - Use it alone: `CA`, `CV`.
      - **Recursive Bayesian filters**, e.g. `Kalman` filters.
      - **Multiple-model** algorithms.

  - Sub-classes: are **_multiple_** `dynamics` motion models considered, or **only _one_**?
  - `1-1.` **Single** model.
    - `kinematic` vs `dynamic` models.
      - Are **forces** that **govern the motion** of other agents considered?
      - Rarely. They are **not directly observable** from sensory data. And `dynamics` models can be very complex.
    - Examples:
      - `CV`, `CA`, **Curvilinear motion** and **Coordinated turn** model (`CT`) (assumes **constant `turn rate`** and `speed` with white noise linear and white noise turn `acceleration`).
    - Miscellaneous:
      - > "The **`bicycle model`** is an often used as an **approximation** to model the **vehicle dynamics** [_rather '`kinematics`' I would have said_]."
      - **Parameters** (e.g. for `IDM`) can be learnt / estimated on-line.

    - _How to improve them and achieve_ **_longer horizons_**_?_ By considering the `static` and `dynamic` environment.
      - `1-` Consider **`map` information**.
        - Simply said: **Roads are structured**. Cars try to **stay on the road**. Considering the **free / drivable space** as **constraints** is therefore beneficial, e.g. to focus on **high-probability regions**.
        Examples:
          - Formalize **relevant _traffic rules_**, e.g. pedestrian crossing permission on the green light, as additional **motion _constraints_**.
          - > "When there are **several possible turns** at a `node`, i.e. at bifurcations, predictions are **propagated** along all outgoing `edges`."
          - **Reachability**-based models.

      - `2-` Consider local **interactions** between multiple agents.
        - **Social force** models. E.g. **potential field** model.
        - **Intelligent Driver Model** (`IDM`) and [`MOBIL`](https://mtreiber.de/publications/MOBIL_TRB.pdf).
        - **Group** motion models: detecting and considering groups of people who walk together.

  - `1-2.` **Multi**-model: the question is _"how to_ **_combine_** _individual models?"_
    - > "Complex agent motion is **poorly described** by a **single dynamical model `f`**. Although the incorporation of **`map`** information and influences from **multiple agents** render such approaches more flexible, they remain inherently limited."
    - Idea:
      - Define (`1`) and **fuse** (`2`) different **prototypical `motion modes`**, each described by a different **dynamic regime `f`**.
       - Example of `modes`: `CA`, `CV`, `turning`, `crossing` or `stopping` (for pedestrians).

    - **_Hybrid_ estimation** and **Multi-Model (`MM`)** methods.
      - Why _"hybrid"_?
        - > "`MM` methods maintain a **hybrid** system `state` `Œæ` = (`x`, `s`) that augments the **continuous valued `x`** by a **discrete-valued `modal` state `s`**."
      - Motion `modes` are **not observable**. **Uncertainty** must be considered. **Believes** must be maintained.
      - Ingredients:
        - `1-` A **`model` set**. Fixed or on-line adaptive.
        - `2-` A strategy to deal with the **_discrete_-valued uncertainties**, for example, **model sequences** under a **Markov** or semi-Markov assumption. 
        - `3-` A **recursive estimation scheme** to deal with the _continuous_ valued components **conditioned on the model**.
        - `4-` A mechanism to **generate the overall best estimate** from a fusion or selection of the individual filter.

      - **_Interactive_ multiple model** filters (`IMM`).
        - _"Interactive"_ because `context`-based **interactions** are considered.
        - > "The [`IMM`-filter](https://onlinelibrary.wiley.com/doi/book/10.1002/0471221279) is a computationally efficient and in many cases well performing **suboptimal estimation** algorithm for **Markovian switching systems**. Basically it consists of `3` major steps:"
          - `interaction` (mixing): Compute an estimate of the `state` with respect to all `N` `transition` models and from these estimates computes `N` mixed inputs.
          - `filtering`: E.g. standard `KF` for each model.
          - `combination`: A **weighted combination** of updated `state` estimates produced by all the filters.
        - E.g. switching between `CV`, constant position (`CP`) and coordinated turn (`CT`).
        - Model `transitions` can be controlled by an **intention recognition** system.
        - `IMM` [can also be used](https://arxiv.org/pdf/1704.04322.pdf) as a **belief updater** in `POMDP`s.
          - It takes as input a `belief state` and an `observation` and returns the updated `belief state`.
          - E.g. to estimate whether the car is following a `CV` or a `CA` model.

    - Other **hybrid** approaches: **stochastic `Reachability` analysis**.
      - > "Model agents as **hybrid systems (with multiple `modes`)** and infer agents' future motions by computing **stochastic reachable sets**."

    - **Non-hybrid**: **Dynamic Bayesian networks (`DBN`)**.
      - Why is it said "_non-hybrid_"? _Not clear to me_
        - Because it does not focus on the **joint (`manoeuvre`, `physical state`) estimation**?
        - As I understand, it is used to infer **unobserved variables** such as `intentions`, `manoeuvre`, or **awareness** of an oncoming vehicle (head orientation).
      - > [Example of `DBN`-based **multi-model**] "Coupling a **set of dynamic systems** (i.e. a bank of Kalman filters (`KF`)) with an `HMM`, which is a special case of the `DBNs`."
      - > "Infer human future behaviors, a set of `macro-actions` described by a set of `KFs`, based on measured dynamic quantities (i.e. `acceleration`, `torque`)."

- `2-` **`Pattern`**-based methods.
  - Idea:
    - **Learn motion `patterns`** from data of **observed agent trajectories**.
    - > "Many `pattern`-based methods **treat agents as particles**, placed in the field of **_learned_ `transitions`**, dictating the direction of future motion."

  - `Supervised` or `unsupervised`? _Not clear to me. Maybe both are possible._
    - It could be `supervised`: instead of manually define (`physics`-based) `x` = `x` + `v`*`dt`, one could **regress** it.
      - **Function approximators** are fit to data: `NN`, `HMM`, `GP`s.
    - It could be `unsupervised`: **manoeuvre recognition** using **clustering** methods on canonical scenarios.

  - Advantages:
    - Ability to **discover statistical behavioural patterns**: `dynamics` functions are not hand-crafted, but rather **approximated** from training data.
  - Limitations:
    - Require **training data**.
    - **Generalization** capability of the learned model: whether it can be **transferred** to a different site, especially if the **`map` topology changes**.
    - > "`Pattern`-based approaches tend to be used in **non-`safety` critical** applications, where **explainability is less of an issue** and where the environment is **spatially constrained**."

  - Two categories. Depending if the **function approximator** makes **temporal factorization** of the `dynamics`:
  - `2-1.` **Sequential** methods.
    - Idea: Learn **conditional models over time** and **_recursively_** apply **learned `transition` functions** for inference.
    - Comparison with many **`physics`-based** approaches:
      - Similarity: they learn a **one-step** predictor `s`.`t+1` = `f`(`s`.`t‚àín:t`, `a.t`).
        - `transition` functions of sequential models have **Markovian property**.
        - > "In order to predict a **sequence of `state` transitions** (i.e. a trajectory), **consecutive one-step predictions** are made to **compose** a single long-term trajectory."
      - Difference: the function, often **non-parametric**, is **learned** from statistical `observations`, and its parameters **cannot be directly interpreted**.

    - Sub-categories, depending on the **`time`-input and `space` range**:
      - `2-1.1` **Local spatial** `transition` patterns.
        - Learning **_local_** motion patterns, such as **probabilities of `transitions` between cells** on a grid-map.
      - `2-1.2` **Location-independent** behavioural patterns.
        - > "Unlike the **_local_ transition** patterns, which are learned and applied for prediction only in a **particular environment**, **_location-independent_** patterns are used for predicting transitions of an agent in **the _general_ free space**."
      - `2-1.3` **Higher-order `Markov`** models.
        - Neural networks for **time series** prediction.
          - Assuming **higher-order Markov property**.
          - > "Such **time series**-based models are making a natural transition between the **first-order `Markovian`** methods (e.g. **_local_ transition** patterns) and **non-sequential** techniques (e.g. **`clustering`-based**)."
        - `LSTM`s are very popular.
          - **`Social`-`LSTM`**, with its **`social pooling` layer**, learns to predict **joint location-independent transitions** in continuous spaces.
          - > "Since humans are influenced by nearby people, `LSTMs` are connected in the **social pooling system**, **sharing information** from the hidden state of the `LSTMs` with the **neighbouring pedestrians**."

  - `2-2.` **Not-sequential** methods.
    - Idea:
      - **Directly** learn a set of **_full_ motion patterns** from data or a **distribution over _trajectories_**.
      - No **temporal factorization** of the `dynamics`, as opposed to **sequential** models (i.e. Markov assumption).
      - > "Specifying **causal constraints**, e.g. `Markovian` assumptions for the **sequential** models or particular functional form for the `physics`-based methods, might be **too restrictive** or require a very large learning dataset."
    - Mainly **unsupervised** with **clustering**-based methods.
      - E.g. with `GPs` or mixture models as **cluster centroids** representation.
      - **Global structure** can be imposed on top of a **sequential model**:
        - > [Example] "Cluster recorded trajectories of humans into **global motion patterns** using the expectation maximization (`EM`) algorithm and build an **`HMM` model for each cluster**. For prediction, the method compares the observed track with the learned motion patterns, and reasons about **which patterns best explain it**. **Uncertainty** is handled by probabilistic mixing of the **most likely patterns**."

- `3-` **`Planning`**-based methods
  - Idea:
    - **`Goal`-informed** predictions: reason on **motion `intent`** of **rational** agents.
    - `1-` **Explicitly reason** about the agent‚Äôs long-term **motion `goals`**.
    - `2-` Compute **`policies`** or **path hypotheses** that enable the agent to **reach these `goals`**.
    - They are intrinsically `map`- and `obstacle`-aware.
    - Requirements:
      - They work well when **`goals`** are **explicitly defined**.
        - > "Most `planning`-based methods rely on a **given set of `goals`**, which makes them unusable or imprecise in a situation where **no `goals` are known beforehand**, or the number of possible `goals` is too high."
      - They often need a **`map` of the environment**.
        - **Potential `goals`** (e.g. exits on a roundabout) in the environment are often used as **prior knowledge**.

    - > "They tend to generate **better long-term predictions** than the `physics`-based techniques and **generalize to new environments** better than the `pattern`-based approaches."

  - They solve **_sequential_** decision making problem:
    - > "Much of the work use **objective functions** that minimizes some notion of the **total `cost` of a sequence** of `actions` (motions), and **not just the `cost` of one `action`** in isolation."

  - Two categories: _Is the `cost` function pre-defined?_
  - `3-1.` "yes": **Forward** planning.
    - **Explicit assumption** regarding the **optimality criteria**.
    - `3-1.1` Motion and **path planning** methods for one **single** observed agent.
      - Use **optimal motion** and **path planning** techniques with a **hand-crafted `cost`-function**.
      - Examples of **classical planning** algorithms: `Dijkstra`, **Fast Marching Method (`FMM`)**, optimal **sampling-based** motion planners, **`value` iteration**.

      - Improvement: **multi-modality**.
        - _We do not know where the observed pedestrian wants to go._
        - Idea: account for **several goals** in the environment.
        - E.g. **Bayesian framework** that exploits the set of **path hypotheses** to estimate the **intended destination**.
          - Hypotheses can be generated from a **Probabilistic Roadmap** (`PRM`).

    - `3-1.2` **Multi-agent** forward planning.
      - Idea: **consider `interactions`** between agents in the scene.
      - E.g. Combine **global planning** (to goals) and **local collision avoidance**.
      - E.g. **cooperative** planning. Consider a **joint `state`-space** that includes all agents.

  - `3-2.` "no": **Inverse planning** methods.
    - Idea:
      - **Estimate the `reward` function** or **`action` model** (`policy`) from observed trajectories.
    - `3-2.1` **Single-agent** inverse planning.
      - **Inverse optimal control** = `IRL`.
      - > [`MaxEnt IRL`] "Humans are assumed to be **near-optimal decision makers** with **stochastic policies**, learned from observations, which are used to predict motion as a **probability distribution** over trajectories."

    - `3-2.2` Directly extract a **`policy`** from the data.
      - **No `reward` function** is explicitly learnt.
      - `GAIL`: A discriminator tries to distinguish between **observations from experts** and generated ones.

      - Deep **generative** techniques, e.g. **[`PRECOG`](https://arxiv.org/abs/1905.01296)** and [`R2P2`](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nicholas_Rhinehart_R2P2_A_ReparameteRized_ECCV_2018_paper.pdf).
      - > "Differently from `GAIL`, the `deep generative technique` by Rhinehart et al. (2018) adopts a fully differentiable model, which is easy to train without the need of an **expensive policy gradient search**."

    - `3-2.3` **Multi-agent** inverse planning.
      - In addition to being **rational**, here, agents are assumed to be (somehow) **cooperative**.
      - > [`DESIRE`](https://arxiv.org/abs/1704.04394) "The method reasons on **multi-modal** future trajectories accounting for **agent interactions**, scene semantics and expected reward function, learned using a **sampling-based `IRL`** scheme. [...] The `RNN` architecture allows **incorporation of past trajectory** into the inference process, which improves prediction accuracy compared to the standard `IRL`-based techniques."

- About **input contextual cues** = factors that **influence pedestrian motion**.
  - `1-` Describing the **target agent**.
    - [History of recent] `position` and `velocity`.
    - Class in {`pedestrian`, `cyclist`}.
    - `attention` and `awareness` of the robot's presence.
    - > "Considering the `head orientation` or full articulated pose of the person may bring valuable insights on the target agent‚Äôs **immediate `intentions`** or their **`awareness` of the environment**."
    - How to deal represent uncertainty? `position`s may come with ellipses denoting uncertainty. How to integrate it?

  - `2-` Describing the **surrounding `dynamic` agents**.
    - **Interaction-`unaware`**: simple `physics`-based method, e.g. `CV`, `CA` models.
    - **`Individual`**-aware:
      - `physics`-based methods with **social forces** or similar **local interaction** models.
      - Many deep learning methods explicitly model interacting entities. E.g. **[`social pooling` layers](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf)**.
      - Learning **`joint` motion patterns** is a considerably harder task.
        - > "Most existing **socially-aware** methods still assume that all observed people are **behaving similarly** and that their motion can be predicted by **the _same_ model** and with the **_same_ features**."
        - > "Most available approaches **assume `cooperative` behavior**, while real humans might rather optimize **personal `goals`** instead of **_joint_ strategies**. In such cases, **game-theoretic** approaches are possibly better suited for modeling human behavior."

    - **`Group`**-aware:
      - Capable of implicitly learning **intra- and inter-group coherence** behaviours.

  - `3-` Describing the **static environment**.
    - > "Humans adapt their behaviors according **not only to the movements of the other agents** but also to the **environment‚Äôs shape and structure**, making extensive use of its **topology** to reason on the possible paths to reach the long-term goal."
    - > "Due to the **high level of structure** in the environment, methods in autonomous driving scenarios **extensively use available `semantic` information**, such as **street layout** and **traffic rules** or current state of the **traffic lights**."
    - (static) `obstacle`-aware and **`map`-aware** methods.
      - Mainly with **occupancy grid maps**. Also **`semantic`** maps.

</details>

---

**`"Rules of the road: Predicting driving behavior with a convolutional model of semantic interactions"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1906.08945)]**
**[** :mortar_board: `Caltech` **]**
**[**:car: `zoox`**]**

- **[** _`multimodal`, `probabilistic`, `1-shot`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/1906.08945).](../media/2019_hong_1.PNG "[Source](https://arxiv.org/abs/1906.08945).")  |
|:--:|
| *The idea is to **encode** a **history of `world states`** (both `static` and `dynamic`) and **semantic `map` information** in a unified, **top-down spatial `grid`**. This allows to use a deep **convolutional** architecture to **model entity dynamics**, **entity interactions**, and **scene context** jointly. The authors found that **temporal convolutions** achieved **better performance** and **significantly faster** training than an `RNN` structure. [Source](https://arxiv.org/abs/1906.08945).* |

| ![[Source](https://arxiv.org/abs/1906.08945).](../media/2019_hong_2.PNG "[Source](https://arxiv.org/abs/1906.08945).")  |
|:--:|
| *Three **`1-shot` models** are proposed. Top: **parametric** and **continuous**. Bottom: **non-parametric** and **discrete** (trajectories are here sampled for display from the `state` probabilities). [Source](https://arxiv.org/abs/1906.08945).* |

Authors: Hong, J., Sapp, B., & Philbin, J.

- Motivations: A prediction method of **future distributions** of **entity `state`** that is:
  - `1-` **Probabilistic**.
    - > "A **single most-likely point estimate** isn't sufficient for a safety-critical system."
    - > "Our perception module also gives us **`state` estimation uncertainty** in the form of **covariance matrices**, and we include this information in our representation via covariance norms."
  - `2-` **Multimodal**.
    - It is important to cover a **diversity of possible implicit actions** an entity might take (e.g., _which way through a junction_).
  - `3-` **One-shot**.
    - It should directly predict **distributions of future states**, rather than **a single point estimate** at **each future timestep**.
    - > "For efficiency reasons, it is desirable to **predict full trajectories** (time sequences of `state` distributions) **without iteratively applying a recurrence step**."
    - > "The problem can be naturally formulated as a **sequence-to-sequence** generation problem. [...] We chose **`‚Ñì=2.5s` of past history**, and predict up to **`m=5s` in the future**."
    - > "**[`DESIRE`](https://arxiv.org/abs/1704.04394)** and **[`R2P2`](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nicholas_Rhinehart_R2P2_A_ReparameteRized_ECCV_2018_paper.pdf)** address **multimodality**, but both do so via **`1`-step stochastic policies**, in contrast to ours which directly predicts a **time sequence** of multimodal distributions. Such **policy-based methods** require both **`future roll-out`** and **`sampling`** to obtain a set of possible trajectories, which has computational trade-offs to our **`one-shot` feed-forward** approach."

- How to model **entity interactions**?
  - `1-` **Implicitly**: By encoding them as **surrounding dynamic context**.
  - `2-` **Explicitly**: For instance, **[`SocialLSTM`](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf) pools hidden temporal `state`** between entity models.
  - Here, **all surrounding entities** are encoded within a **specific** tensor.

- Input:
  - A stack of **`2d`-top-view-grids**. Each frame has **`128√ó128` pixels**, corresponding to **`50m√ó50m`**.
    - For instance, the **dynamic context** is encoded in a `RGB` image with **unique colours** corresponding to each **element type**.
    - The **`state` history** of the considered entity is encoded in a **stack of binary maps**.
      - _One could have use only `1` channel and play with the colour to represent the history_.
  - > "Note that through rendering, we **lose the true graph structure of the road network**, leaving it as a **modeling challenge** to learn **valid road rules** like _legal traffic direction_, and _valid paths through a junction_."
    - _Cannot it just be coded in another tensor?_

- Output. Three approaches are proposed:
  - `1-` **Continuous** + **parametric** representations.
    - `1.1.` A **Gaussian distribution** is **regressed** per future timestep.
    - `1.2.` **Multi-modal** Gaussian Regression (**`GMM-CVAE`**).
      - A **set of Gaussians** is predicted by **sampling from a _categorial_ latent variable**.
      - If non enhanced, this method is naive and suffers from **exchangeability**, and **mode collapse**.
      - > "In general, our **mixture of sampled Gaussian** trajectories **underperformed** our other proposed methods; we observed that some **samples were implausible**."
        - _One could have added an auxiliary loss that penalize off-road predictions, as in the [improved version of `CoverNet`](https://arxiv.org/abs/2006.04767)_
  - `2-` **Discrete** + **non-parametric** representations.
    - Predict **occupancy grid maps**.
      - A **grid** is produced for each future modelled timestep.
      - Each grid location holds the **probability** of the corresponding output `state`.
      - For comparison, trajectories are extracted via some trajectory **sampling procedure**.

- Against **non-learnt baselines**:
  - > "Interestingly, both **`Linear` and `Industry` baselines** performed worse relative to our methods at **larger time offsets**, but **better at smaller offsets**. This can be attributed to the fact that **predicting near futures** can be accurately achieved with **classical physics** (which both baselines leverage) ‚Äî more distant future predictions, however, require more challenging **semantic understanding**."

</details>

---

**`"Learning Interaction-Aware Probabilistic Driver Behavior Models from Urban Scenarios"`**

- **[** `2019` **]**
**[[:memo:](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios)]**
**[** :mortar_board: `TUM` **]**
**[**:car: `BMW`**]**

- **[** _`probabilistic predictions`, `multi-modality`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios).](../media/2019_schulz_2.PNG "[Source](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios).")  |
|:--:|
| *The network produces an **`action` distribution** for the **next time-step**. The `features` are function of one selected **driver's `route intention`** (such as `turning left` or `right`) and the `map`. **Redundant features** can be **pruned** to reduce the complexity of the model: even with as few as **`5` features** (framed in blue), it is possible for the network to learn basic behaviour models that achieve lower losses than both baseline recurrent networks. [Source](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios).* |

| ![[Source](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios).](../media/2019_schulz_1.PNG "[Source](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios).")  |
|:--:|
| *Right: At each time step, the **confidence in the `action` changes**. Left: How to compute some **loss** from the **predicted `variance`** if the **ground-truth** is a **point-estimate**? The predicted distribution can be evaluated at ground-truth, forming a **likelihood**. The **negative-log-likelihood** becomes the objective to minimize. The network can output **high variance** if it is not sure. But a **regularization term** deters it from being too uncertain. [Source](https://www.researchgate.net/publication/334591840_Learning_Interaction-Aware_Probabilistic_Driver_Behavior_Models_from_Urban_Scenarios).* |

Authors: Schulz, J., Hubmann, C., Morin, N., L√∂chner, J., & Darius, B.

- Motivations:
  - `1-` Learn a **driver model** that is:
    - **Probabilistic**. I.e. capture **multi-modality** and **uncertainty** in the predicted low-level actions.
    - **Interaction-aware**. _Well, here the `actions` of surrounding vehicles are ignored, but their `states` are considered_
    - **_"Markovian"_**, i.e. that makes **`1`-step prediction** from the current `state`, assuming independence of previous `state`s / `action`s.
  - `2-` Simplicity + **lightweight**.
    - This model is intended to be integrated as a **probabilistic transition model** into **sampling-based** algorithms, e.g. `particle filtering`.
    - Applications include:
      - `1-` **Forward simulation**-based interaction-aware **`planning` algorithms**, e.g. `Monte Carlo tree search`.
      - `2-` Driver **intention estimation** and **trajectory `prediction`**, here a `DBN` example.
    - Since samples are plenty, **runtime** should be kept low. And therefore, **nested net structures** such as [`DESIRE`](https://arxiv.org/abs/1704.04394) are excluded.
  - Ingredients:
    - **Feedforward net** predicting `steering` and `acceleration` **distributions**.
    - Enable **multi-modality** by building one `input vector`, and making one prediction, per possible `route`.
- About the model:
  - Input: a set of **features** build from:
    - One **route intention**. For instance, the distances of both agents to `entry` and `exit` of the related **conflict areas** are computed.
    - The **map**.
    - The **kinematic state** (`pos`, `heading`, `vel`) of the `2` closest agents.
  - Output: `steering` and `acceleration` **distributions**, modelled as Gaussian: `mean` and `std` are estimated (`cov` = `0`).
    - **Not the next `state`!!**
      - > "Instead of directly learning a **`state` transition model**, we restrict the neural network to learn a **`2`-dimensional `action` distribution** comprising `acceleration` and `steering angle`."
    - Practical implication when building the dataset from real data: the `action`s of **observed** vehicles are unknown, but **inferred using an `inverse bicycle model`**.
  - Using the model at run time:
    - `1-` Sample the **possible `routes`**.
    - `2-` For each route:
      - Start with one `state`.
      - Get one **`action` distribution**. Note that the **uncertainty** can change at each step.
      - **Sample (`acc`, `steer`)** from this distribution.
      - Move to next `state`.
      - Repeat.

- Issue with the **accumulation of `1`-step** to form **long-term predictions**:
  - As in vanilla **imitation learning**, it suffers from **distribution shift** resulting from the **accumulating errors**.
  - > "If this error is too high, the features determined during forward simulation are **not represented** within the training data anymore."
  - A **`DAgger`-like solution** could be considered.

- About **conditioning** on a driver's route intention:
  - Without, one could pack **all the road info** in the input. _How many routes to describe?_ And **expect multiple trajectories** to be produced. _How many output heads?_ Tricky.
  - **Conditioning** offers two advantages:
    - > "The learning algorithm does not have to cope with the **multi-modality** induced by **different route options**. The varying number of **possible `routes`** (depending on the road topology) is handled **outside of the neural network**."
    - It also allows to define _(and detail)_ relevant **features along the considered path**: upcoming `road curvature` or longitudinal distances to `stop lines`.
  - Limit to this approach (again related to the **"off-distribution"** issue):
    - > "When **enumerating all possible routes** and running a **forward simulation** for each of the **conditioned models**, there might exist route candidates that are **so unlikely** that they have **never been followed** in the training data. Thus their features may result in **unreasonable actions** during inference, as the network only learns what actions are **reasonable given a route**, but not which routes are reasonable given a situation."

</details>

---

**`"Learning Predictive Models From Observation and Interaction"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1912.12773)]**
**[[üéûÔ∏è](https://sites.google.com/view/lpmfoai)]**
**[** :mortar_board: `University of Pennsylvania`, `Stanford University`, `UC Berkeley` **]**
**[** :car: `Honda` **]**

- **[** _`visual prediction`, `domain transfer`, `nuScenes`, `BDD100K`_  **]**

<details>
  <summary>Click to expand</summary>

| ![The idea is to learn a **latent representation** **`z`** that corresponds to the **true `action`**. The model can then perform **joint training** on the **two kinds of data**: it optimizes the **likelihood** of the **`interaction`** data, for which the **`action`s are available**, and **`observation`** data, for which the **`action`s are missing**. Hence the visual predictive model can **predict the next frame** `xt+1` conditioned on the **current frame** `xt` and **action learnt representation** `zt`. [Source](https://arxiv.org/abs/1912.12773).](../media/2019_schmeckpeper_1.PNG "The idea is to learn a **latent representation** **`z`** that corresponds to the **true `action`**. The model can then perform **joint training** on the **two kinds of data**: it optimizes the **likelihood** of the **`interaction`** data, for which the **`action`s are available**, and **`observation`** data, for which the **`action`s are missing**. Hence the visual predictive model can **predict the next frame** `xt+1` conditioned on the **current frame** `xt` and **action learnt representation** `zt`. [Source](https://arxiv.org/abs/1912.12773).")  |
|:--:|
| *The idea is to learn a **latent representation** **`z`** that corresponds to the **true `action`**. The model can then perform **joint training** on the **two kinds of data**: it optimizes the **likelihood** of the **`interaction`** data, for which the **`action`s are available**, and **`observation`** data, for which the **`action`s are missing**. Hence the visual predictive model can **predict the next frame** `xt+1` conditioned on the **current frame** `xt` and **action learnt representation** `zt`. [Source](https://arxiv.org/abs/1912.12773).* |

| ![The visual prediction model is trained using two driving sets: **`action`-conditioned videos** from **Boston** and **`action`-free videos** from the **Singapore**. Frames from both subsets come from **[`BDD100K`](https://arxiv.org/abs/1805.04687)** or **[`nuScenes`](https://arxiv.org/abs/1903.11027)** datasets.. [Source](https://sites.google.com/view/lpmfoai).](../media/2019_schmeckpeper_1.gif "The visual prediction model is trained using two driving sets: **`action`-conditioned videos** from **Boston** and **`action`-free videos** from the **Singapore**. Frames from both subsets come from **[`BDD100K`](https://arxiv.org/abs/1805.04687)** or **[`nuScenes`](https://arxiv.org/abs/1903.11027)** datasets.. [Source](https://sites.google.com/view/lpmfoai).")  |
|:--:|
| *The visual prediction model is trained using two driving sets: **`action`-conditioned videos** from **Boston** and **`action`-free videos** from the **Singapore**. Frames from both subsets come from **[`BDD100K`](https://arxiv.org/abs/1805.04687)** and **[`nuScenes`](https://arxiv.org/abs/1903.11027)** datasets. [Source](https://sites.google.com/view/lpmfoai).* |

Authors: Schmeckpeper, K., Xie, A., Rybkin, O., Tian, S., Daniilidis, K., Levine, S., & Finn, C.

- On concrete **industrial use-case**:
  - > "Imagine that a self-driving car company has data from **a fleet of cars** with sensors that record both `video` and the driver‚Äôs `actions` in one city, and a **second fleet of cars** that **only record dashboard `video`**, **without `action`s**, in a second city."
  - > "If the goal is to train an `action`-conditioned model that can be utilized to **predict the outcomes of steering `action`s**, our method allows us to train such a **model using data from both cities**, even though only one of them has `action`s."
- Motivations (_mainly for robotics, but also AD_):
  - Generate **predictions** for complex tasks and **new environments**, without **costly expert demonstrations**.
  - More precisely, learn an `action`-conditioned **video predictive model** from **two kinds of data**:
    - `1-` **_passive_ observations**: [`x0`, `a1`, `x1` ... `aN`, `xN`].
      - Videos of **another agent**, e.g. **a human**, might show the robot how to use a tool.
      - Observations represent a powerful **source of information** about the world and how actions lead to outcomes.
      - A learnt model could also be used for `planning` and `control`, i.e. to **plan** coordinated sequences of actions to bring about **desired outcomes**.
      - But may suffer from **large domain shifts**.
    - `2-` **_active_ interactions**: [`x0`, `x1` ... `xN`].
      - Usually more **expensive**.
- Two challenges:
  - `1-` Observations are **not annotated** with suitable **`action`s**: e.g. only access to the dashcam, not the `throttle` for instance.
    - In other words, `action`s are **only observed** in a **subset** of the data.
    - The goal is to **learn from videos without `action`s**, allowing it to **leverage videos** of agents for which the actions are unknown (**unsupervised** manner).
  - `2-` **Shift** in the **"embodiment"** of the agent: e.g. _robots'_ arms and _humans'_ ones have **physical differences**.
    - The goal is to **bridge the gap** between the **two domains** (e.g., `human arms` vs. `robot arms`).
- _What is learnt?_
  - `p`(`xc+1:T`|`x1:c`, `a1:T`)
  - I.e. **prediction** of **future frames** conditioned on a set of **`c` context frames** and **sequence of actions**.
- _What tests?_
  - `1-` **Different environment** within the **same underlying dataset**: driving in `Boston` and `Singapore`.
  - `2-` **Same environment** but **different embodiment**: `humans` and `robots` manipulate objects with different arms.
- _What is assessed?_
  - `1-` **Prediction** quality (`AD` test).
  - `2-` **Control** performance (`robotics` test).

</details>

---

**`"Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving Applications: A Review"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1912.11676)]**
**[** :mortar_board: `University of Warwick` **]**
**[** :car: `Jaguar Land Rover` **]**

- **[** _`multi-modality prediction`_  **]**

<details>
  <summary>Click to expand</summary>

| ![The author propose **new classification** of behavioural prediction methods. Only **deep learning approaches** are considered and `physics`-based approaches are excluded. The criteria are about the **`input`**, **`ouput`** and **deep learning `method`**. [Source](https://arxiv.org/abs/1912.11676).](../media/2019_mozaffari_1.PNG "The author propose **new classification** of behavioural prediction methods. Only **deep learning approaches** are considered and `physics`-based approaches are excluded. The criteria are about the **`input`**, **`ouput`** and **deep learning `method`**. [Source](https://arxiv.org/abs/1912.11676).")  |
|:--:|
| *The author propose **new classification** of behavioural prediction methods. Only **deep learning approaches** are considered and `physics`-based approaches are excluded. The criteria are about the **`input`**, **`ouput`** and **deep learning `method`**. [Source](https://arxiv.org/abs/1912.11676).* |

| ![First criterion is about the **input**: What is the prediction based on? Important is to capture **road structure** and **interactions** while staying **flexible** in the **representation** (e.g. describe different types of intersections and work with varying numbers of `target vehicles` and `surrounding vehicles`). **Partial observability** should be considered by design. [Source](https://arxiv.org/abs/1912.11676).](../media/2019_mozaffari_2.PNG "First criterion is about the **input**: What is the prediction based on? Important is to capture **road structure** and **interactions** while staying **flexible** in the **representation** (e.g. describe different types of intersections and work with varying numbers of `target vehicles` and `surrounding vehicles`). **Partial observability** should be considered by design. [Source](https://arxiv.org/abs/1912.11676).")  |
|:--:|
| *First criterion is about the **input**: What is the prediction based on? Important is to capture **road structure** and **interactions** while staying **flexible** in the **representation** (e.g. describe different types of intersections and work with varying numbers of `target vehicles` and `surrounding vehicles`). **Partial observability** should be considered by design. [Source](https://arxiv.org/abs/1912.11676).* |

| ![Second criterion is about the **output**: What is predicted? Important is to propagate the uncertainty from the input and **consider multiple options** (**multi-modality**). Therefore to reason with **probabilities**. Bottom - why **`multi-modality`** is important. [Source](https://arxiv.org/abs/1912.11676).](../media/2019_mozaffari_3.PNG "Second criterion is about the **output**: What is predicted? Important is to propagate the uncertainty from the input and **consider multiple options** (**multi-modality**). Therefore to reason with **probabilities**. Bottom - why **`multi-modality`** is important. [Source](https://arxiv.org/abs/1912.11676).")  |
|:--:|
| *Second criterion is about the **output**: What is predicted? Important is to propagate the uncertainty from the input and **consider multiple options** (**multi-modality**). Therefore to reason with **probabilities**. Bottom - why **`multi-modality`** is important. [Source](https://arxiv.org/abs/1912.11676).* |

Authors: Mozaffari, S., Al-Jarrah, O. Y., Dianati, M., Jennings, P., & Mouzakitis, A.

- One mentioned review: [(Lef√®vre et al.)](https://hal.inria.fr/hal-01053736/document) classifies vehicle **(behaviour) prediction models** to three groups:
  - `1-` `physics`-based
    - Use **dynamic** or **kinematic models** of vehicles, e.g. a constant velocity (`CV`) Kalman Filter model.
  - `2-` `manoeuvre`-based
    - Predict vehicles' manoeuvres, i.e. a **classification** problem from a defined **set**.
  - `3-` `interaction`-aware
    - Consider **interaction** of vehicles in the input.
- About the terminology:
  - **_"Target Vehicles"_** (`TV`) are vehicles whose behaviour we are **interested** in predicting.
  - The other are **_"Surrounding Vehicles"_** (`SV`).
  - The **_"Ego Vehicle"_** (`EV`) can be also considered as an `SV`, if it is close enough to `TV`s.
- Here, the authors ignore the `physics`-based methods and propose three criteria for comparison:
  - `1-` **Input**.
    - Track history of `TV` only.
    - Track history of `TV` and `SV`s.
    - Simplified **bird‚Äôs eye view**.
    - Raw sensor data.
  - `2-` **Output**.
    - **Intention** `class`: From a set of pre-defined _discrete classes_, e.g. `go straight`, `turn left`, and `turn right`.
    - **Unimodal** `trajectory`: Usually the one with _highest likelihood_ or the _average_).
    - **Intention-based** `trajectory`: Predict the trajectory that corresponds to the _most probable intention_ (first case).
    - **Multimodal** `trajectory`: Combine the previous ones. Two options, depending if the **intention set** is fixed or dynamically learnt:
      - `static` **intention set**: predict for _each_ member of the set _(an extension to intention-based trajectory prediction approaches)_.
      - `dynamic` **intention set**: due to dynamic definition of manoeuvres, they are prone to _converge to a single manoeuvre_ or not being able to explore all the existing manoeuvres.
  - `3-` In-between (**deep learning method**).
    - `RNN` are used because of their **temporal feature** extracting power.
    - `CNN` are used for their **spatial feature extracting ability (especially with **bird‚Äôs eye views**).
- Important **considerations** for **behavioural prediction**:
  - **Traffic rules**.
  - **Road geometry**.
  - **Multimodality**: there may exist more than one possible future behaviour.
  - **Interaction**.
  - **Uncertainty**: both `aleatoric` (measurement noise) and `epistemic` (partial observability). Hence the prediction should be **probabilistic**.
  - **Prediction horizon**: approaches can serve **different purposes** based on how far in the future they predict (`short-term` or `long-term` future motion).
- Two methods I would like to learn more about:
  - **[`social pooling` layers](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf)**, e.g. used by [(Deo & Trivedi, 2019)](https://arxiv.org/pdf/1805.06771):
    - > "A **social tensor** is a **spatial grid** around the target vehicle that the **occupied cells** are filled with the **processed temporal data** (e.g., `LSTM` hidden state value) of the corresponding vehicle. It contains both the **temporal dynamic** of vehicles represented and **spatial inter-dependencies** among them."
  - **`graph` neural networks**, e.g. [(Diehl et al., 2019)](https://arxiv.org/abs/1903.01254) or [(Li et al., 2019)](https://arxiv.org/abs/1907.07792):
    - **Graph Convolutional Network** (`GCN`).
    - **Graph Attention Network** (`GAT`).
- Comments:
  - Contrary to the _object detection_ task, there is **no benchmark** for **systematically evaluating** previous studies on vehicle behaviour prediction.
    - **Urban scenarios** are excluded in the comparison since `NGSIM I-80` and `US-101 highway` driving datasets are used.
    - _Maybe the [`INTERACTION Dataset‚Äã`](http://interaction-dataset.com/) could be used._
  - The authors suggest **embedding domain knowledge** in the prediction, and call for **practical considerations** (**industry**-supported research).
    - > "Factors such as **environment conditions** and set of **traffic rules** are not directly inputted to the prediction model."
    - > "Practical limitations such as **sensor impairments** and **limited computational** resources have not been fully taken into account."

</details>

---

**`"Multi-Modal Simultaneous Forecasting of Vehicle Position Sequences using Social Attention"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1910.03650)]**
**[** :mortar_board: `Ecole CentraleSupelec` **]**
**[** :car: `Renault` **]**

- **[** _`multi-modality prediction`, `attention mechanism`_  **]**

<details>
  <summary>Click to expand</summary>

| ![Two **multi-head** **`attention`** layers are used to account for **`social` interactions** between all vehicles. They are combined with `LSTM` layers to offer **joint**, **long-range** and **multi-modal** forecasts. [Source](https://arxiv.org/abs/1910.03650).](../media/2019_mercat_2.PNG "Two **multi-head** **`attention`** layers are used to account for **`social` interactions** between all vehicles. They are combined with `LSTM` layers to offer **joint**, **long-range** and **multi-modal** forecasts. [Source](https://arxiv.org/abs/1910.03650).")  |
|:--:|
| *Two **multi-head** **`attention`** layers are used to account for **`social` interactions** between all vehicles. They are combined with `LSTM` layers to offer **joint**, **long-range** and **multi-modal** forecasts. [Source](https://arxiv.org/abs/1910.03650).* |

| ![[Source](https://arxiv.org/abs/1910.03650).](../media/2019_mercat_1.PNG "[Source](https://arxiv.org/abs/1910.03650).")  |
|:--:|
| *[Source](https://arxiv.org/abs/1910.03650).* |

Authors: Mercat, J., Gilles, T., Zoghby, N. El, Sandou, G., Beauvois, D., & Gil, G. P.

- Previous work: `"Social Attention for Autonomous Decision-Making in Dense Traffic"` by (Leurent, & Mercat, 2019), detailed on this page as well.
- Motivations:
  - `1-` **`joint`** - Considering interactions between all vehicles.
  - `2-` **`flexible`** - Independant of the number/order of vehicles.
  - `3-` **`multi-modal`** - Considering uncertainty.
  - `4-` **`long-horizon`** - Predicting over a long range. Here **`5s`** on simple highway scenarios.
  - `5-` **`interpretable`** - E.g. using the **social attention coefficients**.
  - `6-` **`long distance interdependencies`** - The authors decide to exclude the **spatial grid representations** that _"limit the zone of interest to a predefined fixed size and the spatial relation precision to the grid cell size"_.
- Main idea: Stack **`LSTM` layers** with **`social`** **`multi-head`** **`attention`** layers.
  - More precisely, the model is broken into four parts:
    - `1-` An **`Encoder`** processes the sequences of all vehicle **positions** (no information about `speed`, `orientation`, `size` or `blinker`).
    - `2-` A **`Self-attention`** layer captures interactions between all vehicles using "dot product attention". It has **"multiple head"**, each specializing on different **interaction patterns**, e.g. `"closest front vehicle in any lane"`.
    - `3-` A **`Predictor`**, using `LSTM` cells, **forecasts** the positions.
    - A second multi-head self-attention layer is placed here.
    - `4-` A final **`Decoder`** produces sequences of **Gaussians mixtures** for each vehicle.
      - > "What is forecast is not a **mixture of trajectory density functions** but a sequence of **position mixture density functions**. There is a dependency between forecasts at time `tk` and at time `tk+1` but **no explicit link between the modes** at those times."
- Two quotes about multi-modality prediction:
  - > "When considering multiple modes, there is a **challenging trade-off** to find between **anticipating a wide diversity** of modes and **focusing on realistic ones**".
  - > "**`VAE` and `GANs`** are only able to generate an **output distribution** with **sampling** and **do not express a `PDF`**".
- Baselines used to compare the presented **"Social Attention Multi-Modal Prediction"** approach:
  - **Constant velocity** (`CV`), that uses Kalman filters (_hence single modality_).
  - **Convolutional Social Pooling** ([`CSP`](https://arxiv.org/abs/1805.05499)), that uses convolutional social pooling on a **coarse spatial grid**. Six mixture components are used.
  - **Graph-based Interaction-aware Trajectory Prediction** ([`GRIP`](https://arxiv.org/abs/1907.07792)), that uses a **`spatial`** and **`temporal` graph** representation of the scene.

</details>

---

**`"MultiPath : Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1910.05449v1)]**
**[** :car: `Waymo` **]**

- **[** _`anchor`, `multi-modality prediction`, `weighted prediction`, `mode collapse`_  **]**

<details>
  <summary>Click to expand</summary>

| ![[Source](https://arxiv.org/abs/1910.05449v1).](../media/2019_chai_1.PNG "[Source](https://arxiv.org/abs/1910.05449v1).")  |
|:--:|
| *[Source](https://arxiv.org/abs/1910.05449v1).* |

| ![A **discrete set of intents** is modelled as a set of `K=3` **anchor trajectories**. Uncertainty is assumed to be **unimodal given `intent`** (here `3` intents are considered) while `control uncertainty` is modelled with a **Gaussian distribution** dependent on each waypoint state of an anchor trajectory. Such an example shows that modelling **multiple intents** is important. [Source](https://arxiv.org/abs/1910.05449v1).](../media/2019_chai_2.PNG "A **discrete set of intents** is modelled as a set of `K=3` **anchor trajectories**. Uncertainty is assumed to be **unimodal given `intent`** (here `3` intents are considered) while `control uncertainty` is modelled with a **Gaussian distribution** dependent on each waypoint state of an anchor trajectory. Such an example shows that modelling **multiple intents** is important. [Source](https://arxiv.org/abs/1910.05449v1).")  |
|:--:|
| *A **discrete set of intents** is modelled as a set of `K=3` **anchor trajectories**. Uncertainty is assumed to be **unimodal given `intent`** (here `3` intents are considered) while `control uncertainty` is modelled with a **Gaussian distribution** dependent on each waypoint state of an anchor trajectory. Such an example shows that modelling **multiple intents** is important. [Source](https://arxiv.org/abs/1910.05449v1).* |

Authors: Chai, Y., Sapp, B., Bansal, M., & Anguelov, D.

- One idea: **"Anchor Trajectories"**.
  - "Anchor" is a common idea in `ML`. Concrete applications of **"anchor" methods** for `AD` include **`Faster-RCNN` and `YOLO` for object detections**.
    - Instead of directly predicting the size of a bounding box, the `NN` **predicts offsets from a predetermined set of boxes** with particular height-width ratios. Those **predetermined set** of boxes are the **anchor boxes**. (explanation from [this page](https://github.com/pjreddie/darknet/issues/568)).
  - One could therefore draw a parallel between the **_sizes of bounding boxes_** in `Yolo` and the **_shape of trajectories_**: they could be **approximated with some static predetermined patterns** and **refined to the current context** (the actual task of the `NN` here).
    - > "After doing some **clustering studies** on ground truth labels, it turns out that **most bounding boxes have certain height-width ratios**." _[explanation about Yolo from [this page](https://github.com/pjreddie/darknet/issues/568)]_
    - > "Our **trajectory anchors** are **modes found** in our training data in state-sequence space via **unsupervised learning**. These anchors provide **templates for coarse-granularity futures** for an agent and might correspond to **semantic concepts** like `change lanes`, or `slow down`." _[from the presented paper]_
  - This idea reminds also me the concept of **`pre-defined templates`** used for path planning.
- One motivation: model **multiple intents**.
  - This contrasts with the numerous approaches which predict one **single most-likely** trajectory per agent, usually via supervised regression.
  - The **multi-modality** is important since prediction is inherently **stochastic**.
    - The authors distinguish between **`intent uncertainty`** and **`control uncertainty`** (conditioned on intent).
  - A **Gaussian Mixture** Model (`GMM`) distribution is used to model both types of uncertainty.
    - > "At inference, our model predicts a **discrete distribution over the anchors** and, for **each anchor**, regresses offsets from anchor waypoints along with uncertainties, yielding a **Gaussian mixture** at each time step."
- One risk when working with **multi-modality**: directly **learning a mixture** suffers from issues of **"mode collapse"**.
  - This issue is common in `GAN` where the generator starts producing limited varieties of samples.
  - The solution implemented here is to estimate the **anchors** **_a priori_** before **fixing them** to learn the rest of our parameters (as for **`Faster-RCNN`** and **`Yolo`** for instance).
- Second motivation: **weight** the several trajectory predictions.
  - This contrasts with methods that **randomly sample from a generative model** (e.g. **`CVAE`** and **`GAN`**), leading to an **unweighted set** of trajectory samples (not to mention the problem of _reproducibility_ and _analysis_).
  - Here, a **parametric probability distribution** is directly predicted: p(`trajectory`|`observation`), together with a **compact weighted set of explicit trajectories** which **summarizes this distribution well**.
    - This contrasts with methods that outputs a **probabilistic occupancy grid**.
- About the **"top-down" representation**, structured in a **`3d` array**:
  - The first `2` dimensions represent **spatial** locations in the **top-down image**
  - > "The channels in the **depth** dimension hold **`static`** and **time-varying (`dynamic`)** content of a fixed number of previous time steps."
    - **Static context** includes `lane connectivity`, `lane type`, `stop lines`, `speed limit`.
    - **Dynamic context** includes `traffic light states` over the past `5` time-steps.
    - The **previous positions** of the different dynamic objects are also encoded in some **depth channels**.
- One word about the **training dataset**.
  - The model is trained via **`imitation learning`** by fitting the parameters to maximize the **log-likelihood of recorded driving trajectories**.
  - > "The balanced dataset totals **`3.85 million` examples**, contains **`5.75 million` agent trajectories** and constitutes approximately **`200 hours` of (real-world) driving**."

</details>

---

**`"SafeCritic: Collision-Aware Trajectory Prediction"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1910.06673)]**
**[** :mortar_board: `University of Amsterdam` **]**
**[** :car: `BMW` **]**

- **[** _`Conditional GAN`_  **]**

<details>
  <summary>Click to expand</summary>

| ![The **Generator** predicts trajectories that are scored against **two criteria**: The **Discriminator** (as in `GAN`) for **`accuracy`** (i.e. consistent with the observed inputs) and the **Critic** (the generator acts as an **Actor**) for **`safety`**. The random noise vector variable `z` in the Generator can be sampled from `N`(`0`, `1`) to sample novel trajectories. [Source](https://arxiv.org/abs/1909.07707).](../media/2019_van_der_heiden_1.PNG "The **Generator** predicts trajectories that are scored against **two criteria**: The **Discriminator** (as in `GAN`) for **`accuracy`** (i.e. consistent with the observed inputs) and the **Critic** (the generator acts as an **Actor**) for **`safety`**. The random noise vector variable `z` in the Generator can be sampled from `N`(`0`, `1`) to sample novel trajectories. [Source](https://arxiv.org/abs/1909.07707).")  |
|:--:|
| *The **Generator** predicts trajectories that are scored against **two criteria**: The **Discriminator** (as in `GAN`) for **`accuracy`** (i.e. consistent with the observed inputs) and the **Critic** (the generator acts as an **Actor**) for **`safety`**. The random noise vector variable `z` in the Generator can be sampled from `N`(`0`, `1`) to sample novel trajectories. [Source](https://arxiv.org/abs/1909.07707).* |

| ![Several features offered by the predictions of `SafeCritic`: **accuracy**, **diversity**, **attention** and **safety**. [Source](https://arxiv.org/abs/1909.07707).](../media/2019_van_der_heiden_2.PNG "Several features offered by the predictions of `SafeCritic`: **accuracy**, **diversity**, **attention** and **safety**. [Source](https://arxiv.org/abs/1909.07707).")  |
|:--:|
| *Several features offered by the predictions of `SafeCritic`: **accuracy**, **diversity**, **attention** and **safety**. [Source](https://arxiv.org/abs/1909.07707).* |

Authors: van der Heiden, T., Nagaraja, N. S., Weiss, C., & Gavves, E.

- Main motivation:
  - > "We argue that one should take into account `safety`, when designing a model to **predict future trajectories**. Our focus is to generate trajectories that are **not just `accurate`** but also **lead to minimum collisions** and thus are `safe`. Safe trajectories are different from trajectories that **try to imitate** the ground truth, as the latter **may lead to `implausible` paths**, e.g, pedestrians going through walls."
  - Hence the trajectory predictions of the _Generator_ are evaluated against **multiple criteria**:
    - **`Accuracy`**: The _Discriminator_ checks if the prediction is **coherent** / **plausible** with the observation.
    - **`Safety`**: Some _Critic_ predicts the **likelihood** of a future dynamic and static  **collision**.
  - A third loss term is introduced:
    - > "Training the generator is harder than training the discriminator, leading to slow convergence or even failure."
    - An additional **auto-encoding loss** to the ground truth is introduced.
    - It should encourage the model to **avoid trivial solutions** and **mode collapse**, and should **increase the diversity** of future generated trajectories.
    - The term **`mode collapse`** means that instead of suggesting multiple trajectory candidates (`multi-modal`), the model restricts its prediction to only one instance.
- About `RL`:
  - The authors mentioned several terms related to `RL`, in particular they try to dray a parallel with **`Inverse RL`**:
    - > "`GANs` resemble `IRL` in that the **discriminator** learns the **cost function** and the **generator** represents the **policy**."
  - _I got the feeling of that idea, but I was honestly did not understand where it was implemented here. In particular no `MDP` formulation is given_.
- About attention mechanism:
  - > "We rely on attention mechanism for spatial relations in the scene to propose a compact representation for **modelling interaction among all agents** [...] We employ an **attention mechanism** to **prioritize certain elements** in the latent state representations."
  - The _grid-like_ **scene representation** is shared by both the _Generator_ and the _Critic_.
- About the baselines:
  - I like the _"related work"_ section which shortly introduces the state-of-the-art trajectory prediction models based on deep learning. `SafeCritic` takes inspiration from some of their ideas, such as:
    - Aggregation of past information about multiple agents in a **recurrent model**.
    - Use of **Conditional `GAN`** to offer the possibility to also **generate** novel trajectory given observation via **sampling** (standard `GANs` have not encoder).
    - Generation of **multi-modal** future trajectories.
    - Incorporation of **semantic visual** features (extracted by deep networks) combined with an **attention mechanism**.
  - [`SocialGAN`](https://arxiv.org/abs/1803.10892), [`SocialLSTM`](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf), [`Car-Net`](https://arxiv.org/abs/1711.10061), [`SoPhie`](https://arxiv.org/abs/1806.01482) and [`DESIRE`](https://arxiv.org/abs/1704.04394) are used as baselines.
  - [`R2P2`](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nicholas_Rhinehart_R2P2_A_ReparameteRized_ECCV_2018_paper.pdf) and [`SocialAttention`](https://arxiv.org/abs/1710.04689) are also mentioned.

</details>

---

**`"A Review of Tracking, Prediction and Decision Making Methods for Autonomous Driving"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1909.07707)]**
**[** :mortar_board: `University of Iasi` **]**

<details>
  <summary>Click to expand</summary>

One figure:

| ![Classification of __motion models__ based on three increasingly abstract levels - adapted from (Lef√®vre, S., Vasquez. D. & Laugier C. - 2014). [Source](https://arxiv.org/abs/1909.07707).](../media/2019_leon_1.PNG " Classification of motion __motion models__ on three increasingly abstract levels - adapted from (Lef√®vre, S., Vasquez. D. & Laugier C. - 2014). [Source](https://arxiv.org/abs/1909.07707).")  |
|:--:|
| *Classification of __motion models__ based on three increasingly abstract levels - adapted from [(Lef√®vre, S., Vasquez. D. & Laugier C. - 2014)](https://robomechjournal.springeropen.com/track/pdf/10.1186/s40648-014-0001-z). [Source](https://arxiv.org/abs/1909.07707).* |

Authors: Leon, F., & Gavrilescu, M.

- A reference to one white paper: [**"Safety first for automated driving"**](https://www.aptiv.com/docs/default-source/white-papers/safety-first-for-automated-driving-aptiv-white-paper.pdf) 2019 - from Aptiv, Audi, Baidu, BMW, Continental, Daimler, Fiat Chrysler Automobiles, HERE, Infineon, Intel and Volkswagen (alphabetical order). The authors quote some of the good practices about **_Interpretation  and  Prediction_**:
  - Predict only a **short time** into the future (_the further the predicted state is in the future, the less likely it is that the prediction is correct_).
  - Rely on **physics** where possible (_a vehicle driving in front of the automated vehicle will not stop in zero time on its own_).
  - Consider the **compliance** of other road users with traffic rules.
- Miscellaneous notes about prediction:
  - The authors point the need of **high-level reasoning** (the more **abstract** the feature, the more reliable it is **long term**), mentioning both _"affinity"_ and _"attention"_ mechanisms.
  - They also call for **jointly** addressing vehicle **motion modelling** and **risk estimation** (_criticality assessment_).
  - **Gaussian Processed** is found to be a flexible tool for **modelling motion patterns** and is compared to Markov Models for prediction.
    - In particular, GP regressions have the ability to **quantify uncertainty** (e.g. **occlusion**).
  - > "**CNNs** can be **superior to LSTMs** for **temporal modelling** since trajectories are continuous in nature, do not have complicated "state", and have high spatial and temporal correlations".

</details>

---

**`"Deep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules"`**

- **[** `2019` **]**
**[[:memo:](http://cpslab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf)]**
**[[üéûÔ∏è](https://www.youtube.com/watch?v=jpxVm0zL_TM)]**
**[** :mortar_board: `Seoul National University` **]**

<details>
  <summary>Click to expand</summary>

One figure:

| ![The framework consists of four modules: _encoder module_, _interaction module_, _prediction module_ and _control module_. [Source](http://cpslab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf).](../media/2019_cho_1.PNG "The framework consists of four modules: _encoder module_, _interaction module_, _prediction module_ and _control module_. [Source](http://cpslab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf).")  |
|:--:|
| *The framework consists of four modules: _encoder module_, _interaction module_, _prediction module_ and _control module_. [Source](http://cpslab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf).* |

Authors: Cho, K., Ha, T., Lee, G., & Oh, S.

- One previous work: [_"Learning-Based Model Predictive Control under Signal Temporal Logic Specifications"_](http://cpslab.snu.ac.kr/publications/papers/2018_icra_mpcstl.pdf) by (Cho & Ho, 2018).
- One term: **_"robustness slackness"_** for `STL`-formula.
  - The motivation is to solve _dilemma situations_ (inherent to **strict compliance** when all rules cannot be satisfied) by **disobeying certain rules** based on their **predicted degree of satisfaction**.
  - The idea is to **filter out** non-plausible trajectories in the prediction step to only consider **valid prediction candidates** during planning.
  - The filter considers some **"rules"** such as `Lane keeping` and `Collision avoidance of front vehicle` or `Speed limit` (_I did not understand why they are equally considered_).
  - These rules are represented by **Signal Temporal Logic** (`STL`) formulas.
    - Note: `STL` is an extension of _Linear Temporal Logic_ (with _boolean predicates_ and _discrete-time_) with _real-time_ and _real-valued_ constraints.
  - A metric can be introduced to measure how well a given signal (_here, a trajectory candidate_) satisfies a `STL` formula.
    - This is called **_"robustness slackness"_** and acts as a **margin to satisfaction** of `STL`-formula.
  - This enables a **"control under temporal logic specification"** as mentioned by the authors.
- Architecture
  - **Encoder** module: The observed trajectories are fed to some `LSTM` whose internal state is used by the two subsequent modules.
  - **Interaction** module: To consider interaction, all `LSTM` states are **concatenated** (**_joint_** state) together with a feature vector of **relative distances**. In addition, a **CVAE** is used for multi-modality (several possible trajectories are **generated**) and **capture interactions** (_I did not fully understand that point_), as stated by the authors:
    - > "The latent variable `z` models inherent structure in the interaction of multiple vehicles, and it also helps to describe underlying ambiguity of future behaviours of other vehicles."
  - **Prediction** module: Based on the `LSTM` states, the **concatenated vector** and the **latent variable**, both **future trajectories** and **margins to the satisfaction** of each rule are predicted.
  - **Control** module: An `MPC` optimizes the control of the ego car, deciding **which rules should be prioritized** based on the two predicted objects (_trajectories_ and _robustness slackness_).

</details>

---

**`"An Online Evolving Framework for Modeling the Safe Autonomous Vehicle Control System via Online Recognition of Latent Risks"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1908.10823)]**
**[** :mortar_board: `Ohio State University` **]**
**[** :car: `Ford` **]**

- **[** _`MDP`, `action-state transitions matrix`, [`SUMO`](https://sumo.dlr.de/docs/index.html), `risk assessment`_  **]**

<details>
  <summary>Click to expand</summary>

One figure:

| ![Both the **state space** and the **transition model** are adapted online, offering two features: **prediction** about the next state and **detection of unknown (i.e. `risky`) situations**. [Source](https://arxiv.org/abs/1908.10823).](../media/2019_han_1.PNG "Both the **state space** and the **transition model** are adapted online, offering two features: **prediction** about the next state and **detection of unknown (i.e. `risky`) situations**. [Source](https://arxiv.org/abs/1908.10823).")  |
|:--:|
| *Both the **state space** and the **transition model** are adapted online, offering two features: **prediction** about the next state and **detection of unknown (i.e. `risky`) situations**. [Source](https://arxiv.org/abs/1908.10823).* |

Authors: Han, T., Filev, D., & Ozguner, U.

- Motivation
  - _"_**_Rule-based_** and **_supervised-learning_** _methods cannot_ **_recognize unexpected situations_** _so that the AV controller cannot react appropriately under_ **_unknown circumstances_**_."_
  - Based on their previous work on RL [‚ÄúHighway Traffic Modeling and Decision Making for Autonomous Vehicle Using Reinforcement Learning‚Äù](http://dcsl.gatech.edu/papers/iv2018.pdf) by (You, Lu, Filev, & Tsiotras, 2018).
- Main ideas: Both the **state space** and the **transition model** (here discrete state space so transition matrices) of an MDP are **adapted online**.
  - I understand it as trying to **learn the transition model** (experience is generated using `SUMO`), hence to some extent going toward **_model-based RL_**.
  - The motivation is to assist any AV **control framework** with a so-called **_"evolving Finite State Machine"_** (**`e`-`FSM`**).
    - By **identifying state-transitions** precisely, the **future states** can be **predicted**.
    - By determining states uniquely (using **online-clustering** methods) and recognizing the state consistently (expressed by a probability distribution), initially **unexpected dangerous situations** can be detected.
    - It reminds some [ideas about risk assessment](https://github.com/chauvinSimon/IV19#risk-assessment-and-safety-checkers) discussed during IV19: the **discrepancy between expected outcome and observed outcome** is used to **quantify risk**, i.e. the _surprise_ or _misinterpretation_ of the current situation).
- Some concerns:
  - _"The_ **_dimension_** _of transition matrices should_ **_be expanded_** _to represent state-transitions between all existing states"_
    - What when the scenario gets more complex than the presented _"simple car-following"_ and that the **state space** (treated as **_discrete_**) becomes huge?
  - In addition, _"the total **_number of transition matrices_** _is identical to the total_ **_number of actions_**"_.
    - Alone for the simple example, the acceleration command was sampled into `17` bins. **Continuous action spaces** are not an option.

</details>

---

**`"A Driving Intention Prediction Method Based on Hidden Markov Model for Autonomous Driving"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1902.09068)]**
**[** :mortar_board: `IEEE` **]**

- **[** _`HMM`, `Baum-Welch algorithm`, `forward algorithm`_  **]**

<details>
  <summary>Click to expand</summary>

One figure:

| ![[Source](https://arxiv.org/abs/1902.09068).](../media/2019_lui.PNG "[Source](https://arxiv.org/abs/1902.09068).")  |
|:--:|
| *[Source](https://arxiv.org/abs/1902.09068).* |

Authors: Liu, S., Zheng, K., Member, S., Zhao, L., & Fan, P.

- One term: **"mobility feature matrix"**
  - The recorded data (e.g. absolute positions, timestamps ...) are processed to form the _mobility feature matrix_ (e.g. speed, relative position, lateral gap in lane ...).
  - Its size is `T √ó L √ó N`: `T` time steps, `L` vehicles, `N` types of mobility features.
  - In the _discrete characterization_, this matrix is then turned into a **set of observations** using K-means clustering.
  - In the _continuous case_, mobility features are modelled as Gaussian mixture models (GMMs).
- This work implements HMM concepts presented in my project [Educational application of Hidden Markov Model to Autonomous Driving](https://github.com/chauvinSimon/hmm_for_autonomous_driving).

</details>

---

**`"Online Risk-Bounded Motion Planning for Autonomous Vehicles in Dynamic Environments"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1904.02341)]**
**[** :mortar_board: `MIT` **]**
**[** :car: `Toyota` **]**

- **[** _`intention-aware planning`, `manoeuvre-based motion prediction`, `POMDP`, `probabilistic risk assessment`, [`CARLA`](http://carla.org)_ **]**

<details>
  <summary>Click to expand</summary>

One figure:

| ![[Source](https://arxiv.org/abs/1904.02341).](../media/2019_huang.PNG "[Source](https://arxiv.org/abs/1904.02341).")  |
|:--:|
| *[Source](https://arxiv.org/abs/1904.02341).* |

Authors: Huang, X., Hong, S., Hofmann, A., & Williams, B.

- One term: [**"Probabilistic Flow Tubes"**](https://dspace.mit.edu/handle/1721.1/76824) (`PFT`)
  - A *motion representation* used in the **"Motion Model Generator"**.
  - Instead of using **hand-crafted** rules for the transition model, the idea is to **learns human behaviours** from demonstration.
  - The inferred models are encoded with **PFTs** and are used to generate **probabilistic predictions** for both _manoeuvre_ (long-term reasoning) and _motion_ of the other vehicles.
  - The advantage of **belief-based probabilistic planning** is that it can avoid **over-conservative** behaviours while offering **probabilistic safety guarantees**.
- Another term: **"Risk-bounded POMDP Planner"**
  - The **uncertainty** in the intention estimation is then propagated to the decision module.
  - Some notion of **risk**, defined as the _probability of collision_, is evaluated and considered when taking actions, leading to the introduction of a **"chance-constrained POMDP"** (`CC-POMDP`).
  - The **online solver** uses a heuristic-search algorithm, [**Risk-Bounded AO\***](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12321/12095) (**`RAO*`**), takes advantage of the **risk estimation** to prune the over-risky branches that violate the **risk constraints** and eventually outputs a plan with a **guarantee over the probability of success**.
- One quote (_this could apply to many other works_):

> "One possible future work is to test our work in real systems".

</details>

---

**`"Towards Human-Like Prediction and Decision-Making for Automated Vehicles in Highway Scenarios"`**

- **[** `2019` **]**
**[[:memo:](https://tel.archives-ouvertes.fr/tel-02184362/document)]**
**[[:octocat:](https://github.com/marioney/hybrid_simulation/tree/decision-making)]**
**[[üéûÔ∏è](https://www.youtube.com/watch?v=Xx5OmV86CsM)]**
**[** :mortar_board: `INRIA` **]**
**[** :car: `Toyota` **]**

- **[** _`planning-based motion prediction`, `manoeuvre-based motion prediction`_  **]**

<details>
  <summary>Click to expand</summary>

Author: Sierra Gonzalez, D.

- Prediction techniques are often classified into three types:
  - `physics-based`
  - `manoeuvre-based` (and `goal-based`).
  - `interaction-aware`

- As I understood, the main idea here is to **combine prediction techniques** (and their advantages).
  - The **driver-models** (i.e. the reward functions previously learnt with IRL) can be used to identify the most likely, risk-aversive, anticipatory manoeuvres. This is called the `model-based` prediction by the author since it relies on one _model_.
    - But relying only on **driver models** to predict the behaviour of surrounding traffic might fail to predict dangerous manoeuvres.
    - As stated, _"the model-based method is not a reliable alternative for the_ **_short-term_** _estimation of behaviour, since it cannot predict_ **_dangerous actions that deviate_** _from_ **_what is encoded in the model_**_"_.
    - One solution is to add a term that represents **how the observed movement of the target _matches_ a given maneuver**.
    - In other words, to **consider the noisy observation of the dynamics of the targets** and include these so-called `dynamic evidence` into the prediction.

- Usage:
  - The resulting approach is used in the _probabilistic filtering framework_ to **update the belief** in the POMDP and in its **rollout** (to bias the **construction of the history tree** towards likely situations given the state and intention estimations of the surrounding vehicles).
  - It improves the inference of manoeuvres, **reducing rate of false positives** in the detection of `lane change` manoeuvres and enables the exploration of situations in which the surrounding vehicles behave dangerously (not possible if relying on **safe generative models** such as `IDM`).

- One quote about this combination:

> "This model mimics the reasoning process of human drivers: they can guess what a given vehicle is likely to do given the situation (the **model-based prediction**), but they closely **monitor its dynamics** to detect deviations from the expected behaviour".

- One idea: use this combination for **risk assessment**.
  - As stated, _"if the_ **_intended_** _and_ **_expected_** _maneuver of a vehicle_ **_do not match_**_, the situation is classified as dangerous and an alert is triggered"_.
  - This is an important concept of **risk assessment** I could [identify at IV19](https://github.com/chauvinSimon/IV19#risk-assessment-and-safety-checkers): a situation is dangerous if there is a discrepancy between _what is expected_ (given the context) and _what is observed_.

- One term: **"_Interacting Multiple Model_"** (`IMM`), used as baseline in the comparison.
  - The idea is to consider a **group of motion models** (e.g. `lane keeping with CV`, `lane change with CV`) and continuously estimate which of them captures more accurately the dynamics exhibited by the target.
  - The final predictions are produced as a **weighted combination** of the **individual predictions of each filter**.
  - `IMM` belongs to the **_physics-based_** predictions approaches and could be extended for `manoeuvre inference` (called _dynamics matching_). It is often used to **maintain the beliefs** and **guide the observation sampling** in POMDP.
  - But the issue is that IMM completely **disregards the interactions between vehicles**.

</details>

---

**`"Decision making in dynamic and interactive environments based on cognitive hierarchy theory: Formulation, solution, and application to autonomous driving"`**

- **[** `2019` **]**
**[[:memo:](https://arxiv.org/abs/1908.04005)]**
**[** :mortar_board: `University of Michigan` **]**

- **[** _`level-k game theory`, `cognitive hierarchy theory`, `interaction modelling`, `interaction-aware decision making`_  **]**

<details>
  <summary>Click to expand</summary>

Authors: Li, S., Li, N., Girard, A., & Kolmanovsky, I.

- One concept: **`cognitive hierarchy`**.
  - Other drivers are assumed to follow some **"cognitive behavioural models"**, parametrized with a so called **"_cognitive level_"** `œÉ`.
  - The goal is to **obtain and maintain belief about `œÉ`** based on observation in order to **optimally respond** (using an `MPC`).
  - Three levels are considered:
    - level-`0`: driver that treats other vehicles on road as **stationary obstacles**.
    - level-`1`: **cautious/conservative** driver.
    - level-`2`: **aggressive** driver.

- One quote about the **"_cognitive level_"** of human drivers:

> "Humans are most commonly level-1 and level-2 reasoners".

Related works:

- Li, S., Li, N., Girard, A. & Kolmanovsky, I. [2019]. **"Decision making in dynamic and interactive environments based on cognitive hierarchy theory, Bayesian inference, and predictive control"** [[pdf](https://arxiv.org/abs/1908.04005)]
- Li, N., Oyler, D., Zhang, M., Yildiz, Y., Kolmanovsky, I., & Girard, A. [2016]. **"Game-theoretic modeling of driver and vehicle interactions for verification and validation of autonomous vehicle control systems"** [[pdf](https://arxiv.org/abs/1608.08589)]
  - > "If a driver assumes that the other drivers are level-`1` and **takes an action accordingly**, this driver is a level-`2` driver".
  - Use RL with **hierarchical assignment** to learn the policy:
    - First, the `œÄ-0` (for level-`0`) is learnt for the ego-agent.
    - Then `œÄ-1` with all the other participants following `œÄ-0`.
    - Then `œÄ-2` ...
  - **Action masking**: "If a car in the _left lane_ is in a parallel position, the controlled car _cannot change lane to the left_".
    - _"The use of these_ _**hard constrains**_ _eliminates the clearly undesirable behaviours better than through penalizing them in the reward function, and also_ **_increases the learning speed during training_**_"_

- Ren, Y., Elliott, S., Wang, Y., Yang, Y., & Zhang, W. [2019]. **"How Shall I Drive‚ÄØ? Interaction Modeling and Motion Planning towards Empathetic and Socially-Graceful Driving"** [[pdf](https://arxiv.org/abs/1901.10013)] [[code](https://github.com/scaperoth/carmachinelearning)]

| ![[Source](https://arxiv.org/abs/1901.10013).](../media/2019_ren_1.PNG "[Source](https://arxiv.org/abs/1901.10013).")  |
|:--:|
| *[Source](https://arxiv.org/abs/1901.10013).* |

| ![[Source](https://arxiv.org/abs/1901.10013).](../media/2019_ren_2.PNG "[Source](https://arxiv.org/abs/1901.10013).")  |
|:--:|
| *[Source](https://arxiv.org/abs/1901.10013).* |

</details>
